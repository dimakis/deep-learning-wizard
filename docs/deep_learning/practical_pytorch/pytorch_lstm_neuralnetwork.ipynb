{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 8. Long Short-Term Memory (LSTM) network with PyTorch\n",
    "## 1. About LSTMs: Special RNN\n",
    "- Capable of learning long-term dependencies\n",
    "- LSTM = RNN on super juice\n",
    "\n",
    "### 1.1 RNN Transition to LSTM\n",
    "<img src=\"./images/lstm0n2.png\" alt=\"deeplearningwizard\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building an LSTM with PyTorch\n",
    "\n",
    "### Model A: 1 Hidden Layer\n",
    "- Unroll 28 time steps\n",
    "    - Each step input size: 28 x 1\n",
    "    - Total per unroll: 28 x 28\n",
    "        - Feedforward Neural Network input size: 28 x 28 \n",
    "- 1 Hidden layer\n",
    "\n",
    "<img src=\"./images/lstm1.png\" alt=\"deeplearningwizard\" style=\"width: 900px;\"/>\n",
    "\n",
    "### Steps\n",
    "- Step 1: Load Dataset\n",
    "- Step 2: Make Dataset Iterable\n",
    "- Step 3: Create Model Class\n",
    "- Step 4: Instantiate Model Class\n",
    "- Step 5: Instantiate Loss Class\n",
    "- Step 6: Instantiate Optimizer Class\n",
    "- Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Loading MNIST Train Dataset\n",
    "**Images from 1 to 9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.train_data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.train_labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.test_data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.test_labels.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Make Dataset Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # Building your LSTM\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        \n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        \n",
    "        # 28 time steps\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        \n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Instantiate Model Class\n",
    "- 28 time steps\n",
    "    - Each time step: input dimension = 28\n",
    "- 1 hidden layer\n",
    "- MNIST 1-9 digits $\\rightarrow$ output dimension = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "input_dim = 28\n",
    "hidden_dim = 100\n",
    "layer_dim = 1\n",
    "output_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Instantiate Loss Class\n",
    "- Long Short-Term Memory Neural Network: **Cross Entropy Loss**\n",
    "    - _Recurrent Neural Network_: **Cross Entropy Loss**\n",
    "    - _Convolutional Neural Network_: **Cross Entropy Loss**\n",
    "    - _Feedforward Neural Network_: **Cross Entropy Loss**\n",
    "    - _Logistic Regression_: **Cross Entropy Loss**\n",
    "    - _Linear Regression_: **MSE**\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Instantiate Optimizer Class\n",
    "- Simplified equation\n",
    "    - $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta $\n",
    "        - $\\theta$: parameters (our variables)\n",
    "        - $\\eta$: learning rate (how fast we want to learn)\n",
    "        - $\\nabla_\\theta$: parameters' gradients\n",
    "- Even simplier equation\n",
    "    - `parameters = parameters - learning_rate * parameters_gradients`\n",
    "    - **At every iteration, we update our model's parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters In-Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 28])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "- **Input** $\\rightarrow$ **Gates**\n",
    "    - $[400, 28] \\rightarrow w_1, w_3, w_5, w_7$\n",
    "    - $[400] \\rightarrow b_1, b_3, b_5, b_7$\n",
    "- **Hidden State** $\\rightarrow$ **Gates**\n",
    "    - $[400,100] \\rightarrow w_2, w_4, w_6, w_8$\n",
    "    - $[400] \\rightarrow b_2, b_4, b_6, b_8$\n",
    "- **Hidden State** $\\rightarrow$ **Output**\n",
    "    - $[10, 100] \\rightarrow w_9 $\n",
    "    - $[10] \\rightarrow b_9$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/lstm2.png\" alt=\"deeplearningwizard\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train Model\n",
    "- Process \n",
    "    1. **Convert inputs/labels to variables**\n",
    "        - LSTM Input: (1, 28)\n",
    "        - RNN Input: (1, 28)\n",
    "        - CNN Input: (1, 28, 28) \n",
    "        - FNN Input: (1, 28*28)\n",
    "    2. Clear gradient buffets\n",
    "    3. Get output given inputs \n",
    "    4. Get loss\n",
    "    5. Get gradients w.r.t. parameters\n",
    "    6. Update parameters using gradients\n",
    "        - `parameters = parameters - learning_rate * parameters_gradients`\n",
    "    7. REPEAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I put this next model back to what I think is working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.22808392345905304. Accuracy: 11.430000305175781\n",
      "Iteration: 1000. Loss: 0.22944489121437073. Accuracy: 11.350000381469727\n",
      "Iteration: 1500. Loss: 0.2315853089094162. Accuracy: 11.430000305175781\n",
      "Iteration: 2000. Loss: 0.2249087393283844. Accuracy: 19.09000015258789\n",
      "Iteration: 2500. Loss: 0.2318643033504486. Accuracy: 12.630000114440918\n",
      "Iteration: 3000. Loss: 0.231770321726799. Accuracy: 12.770000457763672\n",
      "Iteration: 3500. Loss: 0.22829969227313995. Accuracy: 18.280000686645508\n",
      "Iteration: 4000. Loss: 0.2249705046415329. Accuracy: 16.170000076293945\n",
      "Iteration: 4500. Loss: 0.22302763164043427. Accuracy: 18.280000686645508\n",
      "Iteration: 5000. Loss: 0.2231067419052124. Accuracy: 18.8799991607666\n",
      "Iteration: 5500. Loss: 0.22243599593639374. Accuracy: 18.34000015258789\n",
      "Iteration: 6000. Loss: 0.22698840498924255. Accuracy: 18.15999984741211\n",
      "Iteration: 6500. Loss: 0.2209179401397705. Accuracy: 25.239999771118164\n",
      "Iteration: 7000. Loss: 0.21510593593120575. Accuracy: 27.239999771118164\n",
      "Iteration: 7500. Loss: 0.20170879364013672. Accuracy: 32.06999969482422\n",
      "Iteration: 8000. Loss: 0.1818462610244751. Accuracy: 36.779998779296875\n",
      "Iteration: 8500. Loss: 0.19734880328178406. Accuracy: 46.81999969482422\n",
      "Iteration: 9000. Loss: 0.12727482616901398. Accuracy: 52.279998779296875\n",
      "Iteration: 9500. Loss: 0.13091012835502625. Accuracy: 40.15999984741211\n",
      "Iteration: 10000. Loss: 0.12287384271621704. Accuracy: 54.95000076293945\n",
      "Iteration: 10500. Loss: 0.08425791561603546. Accuracy: 69.12000274658203\n",
      "Iteration: 11000. Loss: 0.06696297228336334. Accuracy: 64.91999816894531\n",
      "Iteration: 11500. Loss: 0.07265748083591461. Accuracy: 68.7300033569336\n",
      "Iteration: 12000. Loss: 0.08000801503658295. Accuracy: 75.87000274658203\n",
      "Iteration: 12500. Loss: 0.11578843742609024. Accuracy: 70.12000274658203\n",
      "Iteration: 13000. Loss: 0.03849148377776146. Accuracy: 67.4000015258789\n",
      "Iteration: 13500. Loss: 0.039198391139507294. Accuracy: 73.56999969482422\n",
      "Iteration: 14000. Loss: 0.10253028571605682. Accuracy: 80.44999694824219\n",
      "Iteration: 14500. Loss: 0.07048767060041428. Accuracy: 81.41999816894531\n",
      "Iteration: 15000. Loss: 0.09074244648218155. Accuracy: 85.9000015258789\n",
      "Iteration: 15500. Loss: 0.021526198834180832. Accuracy: 86.0199966430664\n",
      "Iteration: 16000. Loss: 0.052470434457063675. Accuracy: 86.86000061035156\n",
      "Iteration: 16500. Loss: 0.022132273763418198. Accuracy: 89.11000061035156\n",
      "Iteration: 17000. Loss: 0.016045216470956802. Accuracy: 84.12000274658203\n",
      "Iteration: 17500. Loss: 0.02223769761621952. Accuracy: 89.58999633789062\n",
      "Iteration: 18000. Loss: 0.022599825635552406. Accuracy: 91.19000244140625\n",
      "Iteration: 18500. Loss: 0.01733456924557686. Accuracy: 90.30999755859375\n",
      "Iteration: 19000. Loss: 0.04071715846657753. Accuracy: 91.19000244140625\n",
      "Iteration: 19500. Loss: 0.007999235764145851. Accuracy: 92.5999984741211\n",
      "Iteration: 20000. Loss: 0.047010693699121475. Accuracy: 93.13999938964844\n",
      "Iteration: 20500. Loss: 0.07872595638036728. Accuracy: 91.72000122070312\n",
      "Iteration: 21000. Loss: 0.02588445320725441. Accuracy: 93.68000030517578\n",
      "Iteration: 21500. Loss: 0.029168013483285904. Accuracy: 92.31999969482422\n",
      "Iteration: 22000. Loss: 0.0061218710616230965. Accuracy: 92.31999969482422\n",
      "Iteration: 22500. Loss: 0.038552481681108475. Accuracy: 94.12999725341797\n",
      "Iteration: 23000. Loss: 0.004337577149271965. Accuracy: 94.3499984741211\n",
      "Iteration: 23500. Loss: 0.006641138345003128. Accuracy: 94.27999877929688\n",
      "Iteration: 24000. Loss: 0.0012121433392167091. Accuracy: 95.05999755859375\n",
      "Iteration: 24500. Loss: 0.0024688835255801678. Accuracy: 94.38999938964844\n",
      "Iteration: 25000. Loss: 0.026126781478524208. Accuracy: 93.51000213623047\n",
      "Iteration: 25500. Loss: 0.020766258239746094. Accuracy: 94.58000183105469\n",
      "Iteration: 26000. Loss: 0.0063953339122235775. Accuracy: 94.77999877929688\n",
      "Iteration: 26500. Loss: 0.03189181536436081. Accuracy: 93.0999984741211\n",
      "Iteration: 27000. Loss: 0.001627023913897574. Accuracy: 94.36000061035156\n",
      "Iteration: 27500. Loss: 0.006285636220127344. Accuracy: 95.16999816894531\n",
      "Iteration: 28000. Loss: 0.0032752167899161577. Accuracy: 95.19999694824219\n",
      "Iteration: 28500. Loss: 0.009136824868619442. Accuracy: 95.27999877929688\n",
      "Iteration: 29000. Loss: 0.060894906520843506. Accuracy: 95.18000030517578\n",
      "Iteration: 29500. Loss: 0.01707209274172783. Accuracy: 94.75\n",
      "Iteration: 30000. Loss: 0.03506192937493324. Accuracy: 95.2699966430664\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "accumulation_steps = 10\n",
    "batch_size = 100 // accumulation_steps\n",
    "n_iters = 30000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # Building your LSTM\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        \n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        \n",
    "        # 28 time steps\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        \n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out\n",
    "\n",
    "input_dim = 28\n",
    "hidden_dim = 100\n",
    "layer_dim = 1\n",
    "output_dim = 10\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.1\n",
    "# learning_rate = 0.5\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Number of steps to unroll\n",
    "seq_dim = 28  \n",
    "# accumulation_steps = 10\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as a torch tensor with gradient accumulation abilities\n",
    "        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        # optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size() --> 100, 10\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels) / accumulation_steps\n",
    "\n",
    "        # should be dividing the loss by the accumulation steps here I reckon, but my results work better without this.\n",
    "        \n",
    "        # # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        if ((i+1) % accumulation_steps == 0) or (i + 1 == len(train_dataset)):\n",
    "\n",
    "            # optimizer.zero_grad()\n",
    "\n",
    "            # Getting gradients w.r.t. parameters\n",
    "            # loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Updating parameters\n",
    "        # optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Resize images\n",
    "                images = images.view(-1, seq_dim, input_dim)\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch size 100 // accum_Step = 10\n",
    "\n",
    "number of itrs = 30000\n",
    "\n",
    "Iteration: 500. Loss: 0.552817165851593. Accuracy: 82.41999816894531\n",
    "\n",
    "Iteration: 1000. Loss: 0.12617383897304535. Accuracy: 94.94000244140625\n",
    "\n",
    "Iteration: 1500. Loss: 0.04816253483295441. Accuracy: 97.04000091552734\n",
    "\n",
    "Iteration: 2000. Loss: 0.07681174576282501. Accuracy: 97.75\n",
    "\n",
    "Iteration: 2500. Loss: 0.07728324085474014. Accuracy: 98.08999633789062"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accumulation_steps = 10\n",
    "batch_size = 100 // accumulation_steps\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # Building your LSTM\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        \n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        \n",
    "        # 28 time steps\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        \n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out\n",
    "\n",
    "input_dim = 28\n",
    "hidden_dim = 100\n",
    "layer_dim = 1\n",
    "output_dim = 10\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# learning rate = 0.1\n",
    "learning_rate = 0.5\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Number of steps to unroll\n",
    "seq_dim = 28  \n",
    "# accumulation_steps = 10\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as a torch tensor with gradient accumulation abilities\n",
    "        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        # optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size() --> 100, 10\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # should be dividing the loss by the accumulation steps here I reckon, but my results work better without this.\n",
    "        \n",
    "        # # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        if ((i+1) % accumulation_steps == 0) or (i + 1 == len(train_dataset)):\n",
    "\n",
    "            # optimizer.zero_grad()\n",
    "\n",
    "            # Getting gradients w.r.t. parameters\n",
    "            # loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Updating parameters\n",
    "        # optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Resize images\n",
    "                images = images.view(-1, seq_dim, input_dim)\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the aim:\n",
    "\n",
    "Iteration: 500. Loss: 2.265331506729126. Accuracy: 19.190000534057617\n",
    "\n",
    "Iteration: 1000. Loss: 0.9522631764411926. Accuracy: 66.44000244140625\n",
    "\n",
    "Iteration: 1500. Loss: 0.4156094491481781. Accuracy: 84.1500015258789\n",
    "\n",
    "Iteration: 2000. Loss: 0.2358238250017166. Accuracy: 92.55999755859375\n",
    "\n",
    "Iteration: 2500. Loss: 0.25919219851493835. Accuracy: 91.38999938964844\n",
    "\n",
    "Iteration: 3000. Loss: 0.05853457748889923. Accuracy: 95.70999908447266"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model B: 2 Hidden Layer \n",
    "- Unroll 28 time steps\n",
    "    - Each step input size: 28 x 1\n",
    "    - Total per unroll: 28 x 28\n",
    "        - Feedforward Neural Network inpt size: 28 x 28 \n",
    "- **2 Hidden layer**\n",
    "\n",
    "### Steps\n",
    "- Step 1: Load Dataset\n",
    "- Step 2: Make Dataset Iterable\n",
    "- Step 3: Create Model Class\n",
    "- **Step 4: Instantiate Model Class**\n",
    "- Step 5: Instantiate Loss Class\n",
    "- Step 6: Instantiate Optimizer Class\n",
    "- Step 7: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (lstm): LSTM(28, 100, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "10\n",
      "torch.Size([400, 28])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n",
      "Iteration: 500. Loss: 2.288797378540039. Accuracy: 13.289999961853027\n",
      "Iteration: 1000. Loss: 1.098772406578064. Accuracy: 57.4900016784668\n",
      "Iteration: 1500. Loss: 0.9006298780441284. Accuracy: 77.33999633789062\n",
      "Iteration: 2000. Loss: 0.4344133734703064. Accuracy: 91.08000183105469\n",
      "Iteration: 2500. Loss: 0.2367284893989563. Accuracy: 94.58000183105469\n",
      "Iteration: 3000. Loss: 0.117561474442482. Accuracy: 95.69000244140625\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # Building your LSTM\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        \n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        \n",
    "        # One time step\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        \n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28\n",
    "hidden_dim = 100\n",
    "layer_dim = 2  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\n",
    "output_dim = 10\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "# JUST PRINTING MODEL & PARAMETERS \n",
    "print(model)\n",
    "print(len(list(model.parameters())))\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "\n",
    "# Number of steps to unroll\n",
    "seq_dim = 28  \n",
    "\n",
    "accumulation_steps = 10\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as torch tensor with gradient accumulation abilities\n",
    "        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size() --> 100, 10\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # if ((i+1) % accumulation_steps == 0) or (i + 1 == len(train_dataset)):\n",
    "            # optimizer.step()\n",
    "            # optimizer.zero_grad()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Resize image\n",
    "                images = images.view(-1, seq_dim, input_dim)\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden layer Grad Accum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # Building your LSTM\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        \n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        \n",
    "        # One time step\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        \n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28\n",
    "hidden_dim = 100\n",
    "layer_dim = 2  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\n",
    "output_dim = 10\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "# # JUST PRINTING MODEL & PARAMETERS \n",
    "# print(model)\n",
    "# print(len(list(model.parameters())))\n",
    "# for i in range(len(list(model.parameters()))):\n",
    "#     print(list(model.parameters())[i].size())\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "\n",
    "# Number of steps to unroll\n",
    "seq_dim = 28  \n",
    "\n",
    "accumulation_steps = 10\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as torch tensor with gradient accumulation abilities\n",
    "        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size() --> 100, 10\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # if ((i+1) % accumulation_steps == 0) or (i + 1 == len(train_dataset)):\n",
    "            # optimizer.step()\n",
    "            # optimizer.zero_grad()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Resize image\n",
    "                images = images.view(-1, seq_dim, input_dim)\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters (Layer 1)\n",
    "- **Input** $\\rightarrow$ **Gates**\n",
    "    - $[400, 28]$\n",
    "    - $[400]$\n",
    "- **Hidden State** $\\rightarrow$ **Gates**\n",
    "    - $[400,100]$\n",
    "    - $[400]$\n",
    "   \n",
    "#### Parameters (Layer 2)\n",
    "- **Input** $\\rightarrow$ **Gates** \n",
    "    - $[400, 100]$\n",
    "    - $[400]$\n",
    "- **Hidden State** $\\rightarrow$ **Gates** \n",
    "    - $[400,100]$\n",
    "    - $[400]$\n",
    "   \n",
    "#### Parameters (Readout Layer)\n",
    "- **Hidden State** $\\rightarrow$ **Output**\n",
    "    - $[10, 100]$\n",
    "    - $[10]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model C: 3 Hidden Layer \n",
    "- Unroll 28 time steps\n",
    "    - Each step input size: 28 x 1\n",
    "    - Total per unroll: 28 x 28\n",
    "        - Feedforward Neural Network inpt size: 28 x 28 \n",
    "- **3 Hidden layer**\n",
    "\n",
    "### Steps\n",
    "- Step 1: Load Dataset\n",
    "- Step 2: Make Dataset Iterable\n",
    "- Step 3: Create Model Class\n",
    "- **Step 4: Instantiate Model Class**\n",
    "- Step 5: Instantiate Loss Class\n",
    "- Step 6: Instantiate Optimizer Class\n",
    "- Step 7: Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (lstm): LSTM(28, 100, num_layers=3, batch_first=True)\n",
      "  (fc): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "14\n",
      "torch.Size([400, 28])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n",
      "Iteration: 500. Loss: 2.3018665313720703. Accuracy: 11.350000381469727\n",
      "Iteration: 1000. Loss: 2.2952382564544678. Accuracy: 11.350000381469727\n",
      "Iteration: 1500. Loss: 1.7005929946899414. Accuracy: 39.0099983215332\n",
      "Iteration: 2000. Loss: 0.6987354755401611. Accuracy: 76.45999908447266\n",
      "Iteration: 2500. Loss: 0.4393642842769623. Accuracy: 86.72000122070312\n",
      "Iteration: 3000. Loss: 0.21182510256767273. Accuracy: 94.18000030517578\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "## accumulation_steps\n",
    "accumulation_steps = 10\n",
    "batch_size = 2000 / accumulation_steps\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # Building your LSTM\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        \n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        \n",
    "        # One time step\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        \n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28\n",
    "hidden_dim = 100\n",
    "layer_dim = 3  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\n",
    "output_dim = 10\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "# JUST PRINTING MODEL & PARAMETERS \n",
    "print(model)\n",
    "print(len(list(model.parameters())))\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "\n",
    "# Number of steps to unroll\n",
    "seq_dim = 28  \n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size() --> 100, 10\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters (Layer 1)\n",
    "- **Input** $\\rightarrow$ **Gates**\n",
    "    - [400, 28]\n",
    "    - [400]\n",
    "- **Hidden State** $\\rightarrow$ **Gates**\n",
    "    - [400,100]\n",
    "    - [400]\n",
    "   \n",
    "#### Parameters (Layer 2)\n",
    "- **Input** $\\rightarrow$ **Gates** \n",
    "    - [400, 100]\n",
    "    - [400]\n",
    "- **Hidden State** $\\rightarrow$ **Gates** \n",
    "    - [400,100]\n",
    "    - [400]\n",
    "\n",
    "  \n",
    "\n",
    "#### Parameters (Layer 3)\n",
    "- **Input** $\\rightarrow$ **Gates** \n",
    "    - [400, 100]\n",
    "    - [400]\n",
    "- **Hidden State** $\\rightarrow$ **Gates** \n",
    "    - [400,100]\n",
    "    - [400]\n",
    "\n",
    "    \n",
    "#### Parameters (Readout Layer)\n",
    "- **Hidden State** $\\rightarrow$ **Output**\n",
    "    - [10, 100]\n",
    "    - [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with RNN\n",
    "| Model A RNN | Model B RNN   | Model C RNN | \n",
    "|------|------|\n",
    "|   ReLU | ReLU | Tanh |\n",
    "| 1 Hidden Layer | 2 Hidden Layers | 3 Hidden Layers | \n",
    "| 100 Hidden Units | 100 Hidden Units |100 Hidden Units |\n",
    "| 92.48% | 95.09% | 95.54% | \n",
    "\n",
    "| Model A LSTM | Model B LSTM   | Model C LSTM | \n",
    "|------|------|\n",
    "| 1 Hidden Layer | 2 Hidden Layers | 3 Hidden Layers |\n",
    "| 100 Hidden Units | 100 Hidden Units |100 Hidden Units |\n",
    "| 96.05% | 95.24% | 91.22% | \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning\n",
    "- 2 ways to expand a recurrent neural network\n",
    "    - More hidden units\n",
    "         - `(o, i, f, g) gates`\n",
    "    - More hidden layers\n",
    "- Cons\n",
    "    - Need a larger dataset\n",
    "        - Curse of dimensionality\n",
    "    - Does not necessarily mean higher accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Recurrent Neural Network with PyTorch (GPU)\n",
    "\n",
    "### Model A: 3 Hidden Layers\n",
    "\n",
    "GPU: 2 things must be on GPU\n",
    "- `model`\n",
    "- `tensors`\n",
    "\n",
    "### Steps\n",
    "- Step 1: Load Dataset\n",
    "- Step 2: Make Dataset Iterable\n",
    "- **Step 3: Create Model Class**\n",
    "- **Step 4: Instantiate Model Class**\n",
    "- Step 5: Instantiate Loss Class\n",
    "- Step 6: Instantiate Optimizer Class\n",
    "- **Step 7: Train Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell I will be trying to implement gradient accumulation. With my GPU, a batch size of 7500 will fit into ram but a batch size of 7600 will not. The aim of this report will be to try and make this batch size fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 2.316282272338867. Accuracy: 10.279999732971191. Epoch: 0\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 4         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   18098 KB |  477864 KB |   11212 GB |   11212 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10838 GB |   10838 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     374 GB |     374 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   18098 KB |  477864 KB |   11212 GB |   11212 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10838 GB |   10838 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     374 GB |     374 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  495616 KB |  495616 KB |    1142 MB |  673792 KB |\n",
      "|       from large pool |  487424 KB |  487424 KB |    1124 MB |  663552 KB |\n",
      "|       from small pool |    8192 KB |   10240 KB |      18 MB |   10240 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  100686 KB |  139136 KB |   10048 GB |   10048 GB |\n",
      "|       from large pool |   99437 KB |  137625 KB |    9628 GB |    9628 GB |\n",
      "|       from small pool |    1248 KB |    7823 KB |     419 GB |     419 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      22    |      38    |    4362 K  |    4362 K  |\n",
      "|       from large pool |       4    |      17    |    1043 K  |    1043 K  |\n",
      "|       from small pool |      18    |      34    |    3318 K  |    3318 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      22    |      38    |    4362 K  |    4362 K  |\n",
      "|       from large pool |       4    |      17    |    1043 K  |    1043 K  |\n",
      "|       from small pool |      18    |      34    |    3318 K  |    3318 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       8    |      13    |      22    |      14    |\n",
      "|       from large pool |       4    |       8    |      13    |       9    |\n",
      "|       from small pool |       4    |       5    |       9    |       5    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      15    |      19    |    1995 K  |    1995 K  |\n",
      "|       from large pool |       5    |       7    |     571 K  |     571 K  |\n",
      "|       from small pool |      10    |      14    |    1423 K  |    1423 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "Iteration: 1000. Loss: 2.3039844036102295. Accuracy: 11.350000381469727. Epoch: 0\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 4         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   18098 KB |  477864 KB |   11222 GB |   11222 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10846 GB |   10846 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     376 GB |     376 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   18098 KB |  477864 KB |   11222 GB |   11222 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10846 GB |   10846 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     376 GB |     376 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  495616 KB |  495616 KB |    1142 MB |  673792 KB |\n",
      "|       from large pool |  487424 KB |  487424 KB |    1124 MB |  663552 KB |\n",
      "|       from small pool |    8192 KB |   10240 KB |      18 MB |   10240 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  100686 KB |  139136 KB |   10058 GB |   10058 GB |\n",
      "|       from large pool |   99437 KB |  137625 KB |    9637 GB |    9636 GB |\n",
      "|       from small pool |    1248 KB |    7823 KB |     421 GB |     421 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      22    |      38    |    4385 K  |    4385 K  |\n",
      "|       from large pool |       4    |      17    |    1046 K  |    1046 K  |\n",
      "|       from small pool |      18    |      34    |    3339 K  |    3339 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      22    |      38    |    4385 K  |    4385 K  |\n",
      "|       from large pool |       4    |      17    |    1046 K  |    1046 K  |\n",
      "|       from small pool |      18    |      34    |    3339 K  |    3339 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       8    |      13    |      22    |      14    |\n",
      "|       from large pool |       4    |       8    |      13    |       9    |\n",
      "|       from small pool |       4    |       5    |       9    |       5    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      15    |      19    |    2005 K  |    2005 K  |\n",
      "|       from large pool |       5    |       7    |     574 K  |     574 K  |\n",
      "|       from small pool |      10    |      14    |    1431 K  |    1431 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "Iteration: 1500. Loss: 2.121497392654419. Accuracy: 26.489999771118164. Epoch: 0\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 4         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   18098 KB |  477864 KB |   11231 GB |   11231 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10854 GB |   10854 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     377 GB |     377 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   18098 KB |  477864 KB |   11231 GB |   11231 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10854 GB |   10854 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     377 GB |     377 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  495616 KB |  495616 KB |    1142 MB |  673792 KB |\n",
      "|       from large pool |  487424 KB |  487424 KB |    1124 MB |  663552 KB |\n",
      "|       from small pool |    8192 KB |   10240 KB |      18 MB |   10240 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  100686 KB |  139136 KB |   10067 GB |   10067 GB |\n",
      "|       from large pool |   99437 KB |  137625 KB |    9645 GB |    9644 GB |\n",
      "|       from small pool |    1248 KB |    7823 KB |     422 GB |     422 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      22    |      38    |    4409 K  |    4409 K  |\n",
      "|       from large pool |       4    |      17    |    1049 K  |    1049 K  |\n",
      "|       from small pool |      18    |      34    |    3359 K  |    3359 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      22    |      38    |    4409 K  |    4409 K  |\n",
      "|       from large pool |       4    |      17    |    1049 K  |    1049 K  |\n",
      "|       from small pool |      18    |      34    |    3359 K  |    3359 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       8    |      13    |      22    |      14    |\n",
      "|       from large pool |       4    |       8    |      13    |       9    |\n",
      "|       from small pool |       4    |       5    |       9    |       5    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      15    |      19    |    2015 K  |    2015 K  |\n",
      "|       from large pool |       5    |       7    |     576 K  |     576 K  |\n",
      "|       from small pool |      10    |      14    |    1438 K  |    1438 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "Iteration: 2000. Loss: 1.2767781019210815. Accuracy: 55.869998931884766. Epoch: 0\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 4         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   18098 KB |  477864 KB |   11240 GB |   11240 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10862 GB |   10862 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     378 GB |     378 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   18098 KB |  477864 KB |   11240 GB |   11240 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10862 GB |   10862 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     378 GB |     378 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  495616 KB |  495616 KB |    1142 MB |  673792 KB |\n",
      "|       from large pool |  487424 KB |  487424 KB |    1124 MB |  663552 KB |\n",
      "|       from small pool |    8192 KB |   10240 KB |      18 MB |   10240 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  100686 KB |  139136 KB |   10077 GB |   10077 GB |\n",
      "|       from large pool |   99437 KB |  137625 KB |    9653 GB |    9652 GB |\n",
      "|       from small pool |    1248 KB |    7823 KB |     424 GB |     424 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      22    |      38    |    4432 K  |    4432 K  |\n",
      "|       from large pool |       4    |      17    |    1052 K  |    1052 K  |\n",
      "|       from small pool |      18    |      34    |    3380 K  |    3380 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      22    |      38    |    4432 K  |    4432 K  |\n",
      "|       from large pool |       4    |      17    |    1052 K  |    1052 K  |\n",
      "|       from small pool |      18    |      34    |    3380 K  |    3380 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       8    |      13    |      22    |      14    |\n",
      "|       from large pool |       4    |       8    |      13    |       9    |\n",
      "|       from small pool |       4    |       5    |       9    |       5    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      15    |      19    |    2025 K  |    2025 K  |\n",
      "|       from large pool |       5    |       7    |     578 K  |     578 K  |\n",
      "|       from small pool |      10    |      14    |    1446 K  |    1446 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "Iteration: 2500. Loss: 0.7293757200241089. Accuracy: 69.93000030517578. Epoch: 0\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 4         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   18098 KB |  477864 KB |   11250 GB |   11250 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10870 GB |   10870 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     379 GB |     379 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   18098 KB |  477864 KB |   11250 GB |   11250 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10870 GB |   10870 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     379 GB |     379 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  495616 KB |  495616 KB |    1142 MB |  673792 KB |\n",
      "|       from large pool |  487424 KB |  487424 KB |    1124 MB |  663552 KB |\n",
      "|       from small pool |    8192 KB |   10240 KB |      18 MB |   10240 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  100686 KB |  139136 KB |   10086 GB |   10086 GB |\n",
      "|       from large pool |   99437 KB |  137625 KB |    9661 GB |    9660 GB |\n",
      "|       from small pool |    1248 KB |    7823 KB |     425 GB |     425 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      22    |      38    |    4456 K  |    4456 K  |\n",
      "|       from large pool |       4    |      17    |    1055 K  |    1055 K  |\n",
      "|       from small pool |      18    |      34    |    3400 K  |    3400 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      22    |      38    |    4456 K  |    4456 K  |\n",
      "|       from large pool |       4    |      17    |    1055 K  |    1055 K  |\n",
      "|       from small pool |      18    |      34    |    3400 K  |    3400 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       8    |      13    |      22    |      14    |\n",
      "|       from large pool |       4    |       8    |      13    |       9    |\n",
      "|       from small pool |       4    |       5    |       9    |       5    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      15    |      19    |    2035 K  |    2035 K  |\n",
      "|       from large pool |       5    |       7    |     580 K  |     580 K  |\n",
      "|       from small pool |      10    |      14    |    1454 K  |    1454 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "Iteration: 3000. Loss: 0.47363418340682983. Accuracy: 78.63999938964844. Epoch: 0\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 4         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   18098 KB |  477864 KB |   11259 GB |   11259 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10878 GB |   10878 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     381 GB |     381 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   18098 KB |  477864 KB |   11259 GB |   11259 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10878 GB |   10878 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     381 GB |     381 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  495616 KB |  495616 KB |    1142 MB |  673792 KB |\n",
      "|       from large pool |  487424 KB |  487424 KB |    1124 MB |  663552 KB |\n",
      "|       from small pool |    8192 KB |   10240 KB |      18 MB |   10240 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  100686 KB |  139136 KB |   10096 GB |   10095 GB |\n",
      "|       from large pool |   99437 KB |  137625 KB |    9669 GB |    9668 GB |\n",
      "|       from small pool |    1248 KB |    7823 KB |     426 GB |     426 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      22    |      38    |    4479 K  |    4479 K  |\n",
      "|       from large pool |       4    |      17    |    1058 K  |    1058 K  |\n",
      "|       from small pool |      18    |      34    |    3421 K  |    3421 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      22    |      38    |    4479 K  |    4479 K  |\n",
      "|       from large pool |       4    |      17    |    1058 K  |    1058 K  |\n",
      "|       from small pool |      18    |      34    |    3421 K  |    3421 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       8    |      13    |      22    |      14    |\n",
      "|       from large pool |       4    |       8    |      13    |       9    |\n",
      "|       from small pool |       4    |       5    |       9    |       5    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      15    |      19    |    2045 K  |    2045 K  |\n",
      "|       from large pool |       5    |       7    |     583 K  |     583 K  |\n",
      "|       from small pool |      10    |      14    |    1461 K  |    1461 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "Iteration: 3500. Loss: 0.6380165219306946. Accuracy: 59.54999923706055. Epoch: 1\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 4         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   18098 KB |  477864 KB |   11268 GB |   11268 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10886 GB |   10886 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     382 GB |     382 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   18098 KB |  477864 KB |   11268 GB |   11268 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10886 GB |   10886 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     382 GB |     382 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  495616 KB |  495616 KB |    1142 MB |  673792 KB |\n",
      "|       from large pool |  487424 KB |  487424 KB |    1124 MB |  663552 KB |\n",
      "|       from small pool |    8192 KB |   10240 KB |      18 MB |   10240 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  100686 KB |  139136 KB |   10105 GB |   10105 GB |\n",
      "|       from large pool |   99437 KB |  137625 KB |    9677 GB |    9677 GB |\n",
      "|       from small pool |    1248 KB |    7823 KB |     428 GB |     428 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      22    |      38    |    4503 K  |    4503 K  |\n",
      "|       from large pool |       4    |      17    |    1061 K  |    1061 K  |\n",
      "|       from small pool |      18    |      34    |    3441 K  |    3441 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      22    |      38    |    4503 K  |    4503 K  |\n",
      "|       from large pool |       4    |      17    |    1061 K  |    1061 K  |\n",
      "|       from small pool |      18    |      34    |    3441 K  |    3441 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       8    |      13    |      22    |      14    |\n",
      "|       from large pool |       4    |       8    |      13    |       9    |\n",
      "|       from small pool |       4    |       5    |       9    |       5    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      15    |      19    |    2054 K  |    2054 K  |\n",
      "|       from large pool |       5    |       7    |     585 K  |     585 K  |\n",
      "|       from small pool |      10    |      14    |    1469 K  |    1469 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "Iteration: 4000. Loss: 0.26775944232940674. Accuracy: 89.05000305175781. Epoch: 1\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 4         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   18098 KB |  477864 KB |   11278 GB |   11277 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10894 GB |   10894 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     383 GB |     383 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   18098 KB |  477864 KB |   11278 GB |   11277 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10894 GB |   10894 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     383 GB |     383 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  495616 KB |  495616 KB |    1142 MB |  673792 KB |\n",
      "|       from large pool |  487424 KB |  487424 KB |    1124 MB |  663552 KB |\n",
      "|       from small pool |    8192 KB |   10240 KB |      18 MB |   10240 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  100686 KB |  139136 KB |   10114 GB |   10114 GB |\n",
      "|       from large pool |   99437 KB |  137625 KB |    9685 GB |    9685 GB |\n",
      "|       from small pool |    1248 KB |    7823 KB |     429 GB |     429 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      22    |      38    |    4526 K  |    4526 K  |\n",
      "|       from large pool |       4    |      17    |    1064 K  |    1064 K  |\n",
      "|       from small pool |      18    |      34    |    3462 K  |    3462 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      22    |      38    |    4526 K  |    4526 K  |\n",
      "|       from large pool |       4    |      17    |    1064 K  |    1064 K  |\n",
      "|       from small pool |      18    |      34    |    3462 K  |    3462 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       8    |      13    |      22    |      14    |\n",
      "|       from large pool |       4    |       8    |      13    |       9    |\n",
      "|       from small pool |       4    |       5    |       9    |       5    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      15    |      19    |    2064 K  |    2064 K  |\n",
      "|       from large pool |       5    |       7    |     587 K  |     587 K  |\n",
      "|       from small pool |      10    |      14    |    1477 K  |    1477 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "Iteration: 4500. Loss: 0.07055635750293732. Accuracy: 91.27999877929688. Epoch: 1\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 4         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   18098 KB |  477864 KB |   11287 GB |   11287 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10902 GB |   10902 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     384 GB |     384 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   18098 KB |  477864 KB |   11287 GB |   11287 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10902 GB |   10902 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     384 GB |     384 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  495616 KB |  495616 KB |    1142 MB |  673792 KB |\n",
      "|       from large pool |  487424 KB |  487424 KB |    1124 MB |  663552 KB |\n",
      "|       from small pool |    8192 KB |   10240 KB |      18 MB |   10240 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  100686 KB |  139136 KB |   10124 GB |   10124 GB |\n",
      "|       from large pool |   99437 KB |  137625 KB |    9693 GB |    9693 GB |\n",
      "|       from small pool |    1248 KB |    7823 KB |     431 GB |     431 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      22    |      38    |    4550 K  |    4550 K  |\n",
      "|       from large pool |       4    |      17    |    1067 K  |    1067 K  |\n",
      "|       from small pool |      18    |      34    |    3482 K  |    3482 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      22    |      38    |    4550 K  |    4550 K  |\n",
      "|       from large pool |       4    |      17    |    1067 K  |    1067 K  |\n",
      "|       from small pool |      18    |      34    |    3482 K  |    3482 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       8    |      13    |      22    |      14    |\n",
      "|       from large pool |       4    |       8    |      13    |       9    |\n",
      "|       from small pool |       4    |       5    |       9    |       5    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      15    |      19    |    2074 K  |    2074 K  |\n",
      "|       from large pool |       5    |       7    |     589 K  |     589 K  |\n",
      "|       from small pool |      10    |      14    |    1484 K  |    1484 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "Iteration: 5000. Loss: 0.7460951805114746. Accuracy: 94.27999877929688. Epoch: 1\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 4         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   18098 KB |  477864 KB |   11296 GB |   11296 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10910 GB |   10910 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     386 GB |     386 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   18098 KB |  477864 KB |   11296 GB |   11296 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10910 GB |   10910 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     386 GB |     386 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  495616 KB |  495616 KB |    1142 MB |  673792 KB |\n",
      "|       from large pool |  487424 KB |  487424 KB |    1124 MB |  663552 KB |\n",
      "|       from small pool |    8192 KB |   10240 KB |      18 MB |   10240 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  100686 KB |  139136 KB |   10133 GB |   10133 GB |\n",
      "|       from large pool |   99437 KB |  137625 KB |    9701 GB |    9701 GB |\n",
      "|       from small pool |    1248 KB |    7823 KB |     432 GB |     432 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      22    |      38    |    4573 K  |    4573 K  |\n",
      "|       from large pool |       4    |      17    |    1070 K  |    1070 K  |\n",
      "|       from small pool |      18    |      34    |    3503 K  |    3503 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      22    |      38    |    4573 K  |    4573 K  |\n",
      "|       from large pool |       4    |      17    |    1070 K  |    1070 K  |\n",
      "|       from small pool |      18    |      34    |    3503 K  |    3503 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       8    |      13    |      22    |      14    |\n",
      "|       from large pool |       4    |       8    |      13    |       9    |\n",
      "|       from small pool |       4    |       5    |       9    |       5    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      15    |      19    |    2084 K  |    2084 K  |\n",
      "|       from large pool |       5    |       7    |     592 K  |     592 K  |\n",
      "|       from small pool |      10    |      14    |    1492 K  |    1492 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "Iteration: 5500. Loss: 0.38634753227233887. Accuracy: 94.95999908447266. Epoch: 1\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 4         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   18098 KB |  477864 KB |   11305 GB |   11305 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10918 GB |   10918 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     387 GB |     387 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   18098 KB |  477864 KB |   11305 GB |   11305 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10918 GB |   10918 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     387 GB |     387 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  495616 KB |  495616 KB |    1142 MB |  673792 KB |\n",
      "|       from large pool |  487424 KB |  487424 KB |    1124 MB |  663552 KB |\n",
      "|       from small pool |    8192 KB |   10240 KB |      18 MB |   10240 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  100686 KB |  139136 KB |   10143 GB |   10143 GB |\n",
      "|       from large pool |   99437 KB |  137625 KB |    9709 GB |    9709 GB |\n",
      "|       from small pool |    1248 KB |    7823 KB |     434 GB |     434 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      22    |      38    |    4597 K  |    4597 K  |\n",
      "|       from large pool |       4    |      17    |    1073 K  |    1073 K  |\n",
      "|       from small pool |      18    |      34    |    3523 K  |    3523 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      22    |      38    |    4597 K  |    4597 K  |\n",
      "|       from large pool |       4    |      17    |    1073 K  |    1073 K  |\n",
      "|       from small pool |      18    |      34    |    3523 K  |    3523 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       8    |      13    |      22    |      14    |\n",
      "|       from large pool |       4    |       8    |      13    |       9    |\n",
      "|       from small pool |       4    |       5    |       9    |       5    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      15    |      19    |    2094 K  |    2094 K  |\n",
      "|       from large pool |       5    |       7    |     594 K  |     594 K  |\n",
      "|       from small pool |      10    |      14    |    1500 K  |    1500 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "Iteration: 6000. Loss: 0.19910956919193268. Accuracy: 95.83000183105469. Epoch: 1\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 4         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   18098 KB |  477864 KB |   11315 GB |   11315 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10926 GB |   10926 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     388 GB |     388 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   18098 KB |  477864 KB |   11315 GB |   11315 GB |\n",
      "|       from large pool |   15250 KB |  474448 KB |   10926 GB |   10926 GB |\n",
      "|       from small pool |    2847 KB |    7400 KB |     388 GB |     388 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  495616 KB |  495616 KB |    1142 MB |  673792 KB |\n",
      "|       from large pool |  487424 KB |  487424 KB |    1124 MB |  663552 KB |\n",
      "|       from small pool |    8192 KB |   10240 KB |      18 MB |   10240 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  100686 KB |  139136 KB |   10152 GB |   10152 GB |\n",
      "|       from large pool |   99437 KB |  137625 KB |    9717 GB |    9717 GB |\n",
      "|       from small pool |    1248 KB |    7823 KB |     435 GB |     435 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      22    |      38    |    4620 K  |    4620 K  |\n",
      "|       from large pool |       4    |      17    |    1076 K  |    1076 K  |\n",
      "|       from small pool |      18    |      34    |    3544 K  |    3544 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      22    |      38    |    4620 K  |    4620 K  |\n",
      "|       from large pool |       4    |      17    |    1076 K  |    1076 K  |\n",
      "|       from small pool |      18    |      34    |    3544 K  |    3544 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       8    |      13    |      22    |      14    |\n",
      "|       from large pool |       4    |       8    |      13    |       9    |\n",
      "|       from small pool |       4    |       5    |       9    |       5    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      15    |      19    |    2104 K  |    2104 K  |\n",
      "|       from large pool |       5    |       7    |     596 K  |     596 K  |\n",
      "|       from small pool |      10    |      14    |    1507 K  |    1507 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracyArr = []\n",
    "train_losses = []\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(10,5))\n",
    "# plt.title(\"Training and Validation Loss\")\n",
    "# plt.plot(val_losses,label=\"val\")\n",
    "# plt.plot(train_losses,label=\"train\")\n",
    "# plt.xlabel(\"iterations\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# writer = SummaryWriter()\n",
    "\n",
    "# torch.cuda.set_per_process_memory_fraction(0.5, 0)\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                        train=False, \n",
    "                        transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "accumulation_steps = 5\n",
    "batch_size = 20\n",
    "#0 // accumulation_steps\n",
    "# n_iters = 5000\n",
    "n_iters = 8000\n",
    "## note the len of the train_dataset is 60000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                        batch_size=batch_size, \n",
    "                                        shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                        batch_size=batch_size, \n",
    "                                        shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # Building your LSTM\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        #######################\n",
    "        #  USE GPU FOR MODEL  #\n",
    "        #######################\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        \n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        \n",
    "        # One time step\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        \n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28\n",
    "hidden_dim = 100\n",
    "layer_dim = 3  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\n",
    "output_dim = 10\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "    \n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "# learning_rate = 0.5\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "\n",
    "# Number of steps to unroll\n",
    "seq_dim = 28  \n",
    "\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        #######################\n",
    "        #  USE GPU FOR MODEL  #\n",
    "        #######################\n",
    "        images = images.view(-1, seq_dim, input_dim).requires_grad_().to(device)\n",
    "        labels = labels.to(device)\n",
    "            \n",
    "        # # Clear gradients w.r.t. parameters\n",
    "        # optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size() --> 100, 10\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels) #/ accumulation_steps\n",
    "        \n",
    "        ## this is potentially needed to normalize our loss if our loss is averaged\n",
    "        # loss = loss / accumulation_steps\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        if ((i + 1) % accumulation_steps == 0) or (i + 1 == len(train_dataset)):\n",
    "        # if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Updating parameters\n",
    "        # optimizer.step()\n",
    "        # optimizer.zero_grad()\n",
    "        \n",
    "        iter += 1\n",
    "            \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                images = images.view(-1, seq_dim, input_dim).to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            accuracyArr.append(accuracy)\n",
    "\n",
    "            # for use with tensorboard, not working\n",
    "            # writer.add_scalar('Loss/train', loss.item(), iter)\n",
    "            # writer.add_scalar('Accuracy/train', accuracy, iter)\n",
    "            # writer.close()\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}. Epoch: {}'.format(iter, loss.item(), accuracy, epoch))\n",
    "            print(torch.cuda.memory_summary(abbreviated=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without div\n",
    "Iteration: 500. Loss: 2.290450096130371. Accuracy: 11.350000381469727\n",
    "\n",
    "Iteration: 1000. Loss: 2.300975799560547. Accuracy: 11.350000381469727\n",
    "\n",
    "Iteration: 1500. Loss: 2.2891440391540527. Accuracy: 11.350000381469727\n",
    "\n",
    "Iteration: 2000. Loss: 2.298623561859131. Accuracy: 20.040000915527344\n",
    "\n",
    "Iteration: 2500. Loss: 1.5900005102157593. Accuracy: 42.11000061035156\n",
    "\n",
    "Iteration: 3000. Loss: 1.3344417810440063. Accuracy: 58.38999938964844"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Iterations (s)')\n",
    "ax1.set_ylabel('Accuracy', color=color)\n",
    "ax1.plot(accuracyArr, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Loss %', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(train_losses, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.plot(accuracyArr, label=\"accuracy\")\n",
    "plt.plot(train_losses,label=\"loss\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grad_accu without loss / by accum steps\n",
    "\n",
    "Iteration: 500. Loss: 2.3048462867736816. Accuracy: 11.350000381469727\n",
    "\n",
    "Iteration: 1000. Loss: 2.29508638381958. Accuracy: 11.350000381469727\n",
    "\n",
    "Iteration: 1500. Loss: 2.2026185989379883. Accuracy: 24.709999084472656\n",
    "\n",
    "Iteration: 2000. Loss: 2.4915976524353027. Accuracy: 7.699999809265137\n",
    "\n",
    "Iteration: 2500. Loss: 1.0642483234405518. Accuracy: 61.38999938964844\n",
    "\n",
    "Iteration: 3000. Loss: 0.7956912517547607. Accuracy: 75.81999969482422\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grad accum with / by accum steps\n",
    "\n",
    "Iteration: 500. Loss: 0.2304486334323883. Accuracy: 11.350000381469727\n",
    "\n",
    "Iteration: 1000. Loss: 0.23037873208522797. Accuracy: 11.350000381469727\n",
    "\n",
    "Iteration: 1500. Loss: 0.22993922233581543. Accuracy: 11.350000381469727\n",
    "\n",
    "Iteration: 2000. Loss: 0.23011620342731476. Accuracy: 11.350000381469727"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the final non grad accum that I am using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_99266/67326346.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m '''\n",
      "\u001b[0;32m~/anaconda3/envs/dis-sys_lstm/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/anaconda3/envs/dis-sys_lstm/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dis-sys_lstm/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNNBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Resets _flat_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dis-sys_lstm/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dis-sys_lstm/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracyArrNoGradAccum = []\n",
    "train_lossesNoGradAccum = []\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(10,5))\n",
    "# plt.title(\"Training and Validation Loss\")\n",
    "# plt.plot(val_losses,label=\"val\")\n",
    "# plt.plot(train_losses,label=\"train\")\n",
    "# plt.xlabel(\"iterations\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# writer = SummaryWriter()\n",
    "\n",
    "# torch.cuda.set_per_process_memory_fraction(0.5, 0)\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                        train=False, \n",
    "                        transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "# accumulation_steps = 10\n",
    "batch_size = 32\n",
    "# n_iters = 100\n",
    "n_iters = 3000\n",
    "## note the len of the train_dataset is 60000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                        batch_size=batch_size, \n",
    "                                        shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                        batch_size=batch_size, \n",
    "                                        shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # Building your LSTM\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        #######################\n",
    "        #  USE GPU FOR MODEL  #\n",
    "        #######################\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        \n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        \n",
    "        # One time step\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        \n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28\n",
    "hidden_dim = 100\n",
    "layer_dim = 3  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\n",
    "output_dim = 10\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "    \n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "\n",
    "# Number of steps to unroll\n",
    "seq_dim = 28  \n",
    "\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        #######################\n",
    "        #  USE GPU FOR MODEL  #\n",
    "        #######################\n",
    "        images = images.view(-1, seq_dim, input_dim).requires_grad_().to(device)\n",
    "        labels = labels.to(device)\n",
    "            \n",
    "        # # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size() --> 100, 10\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels) \n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # if ((i + 1) % accumulation_steps == 0) or (i + 1 == len(train_dataset)):\n",
    "        # # if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "        #     optimizer.step()\n",
    "        #     optimizer.zero_grad()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "            \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                images = images.view(-1, seq_dim, input_dim).to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            train_lossesNoGradAccum.append(loss.item())\n",
    "\n",
    "            accuracyArrNoGradAccum.append(accuracy)\n",
    "\n",
    "            # for use with tensorboard, not working\n",
    "            # writer.add_scalar('Loss/train', loss.item(), iter)\n",
    "            # writer.add_scalar('Accuracy/train', accuracy, iter)\n",
    "            # writer.close()\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}. Epoch: {}'.format(iter, loss.item(), accuracy, epoch))\n",
    "            print(torch.cuda.memory_summary(abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAA89klEQVR4nO3deZxT1d3H8c9JZt+HfZgZyCBoLoLsuG+1KhA2tXVr3WqLtra22sWpbZ+2tk+fae1mF7XWfUOtgMwQFBUR17KjIDcoQmTYl2H2JZPkPH8kgwOyDJDkJpnf+9W8kntzl99Une/cc889R2mtEUIIIeKNzeoChBBCiEORgBJCCBGXJKCEEELEJQkoIYQQcUkCSgghRFySgBJCCBGXUqJ1YNNpPApMBnYZHnNYeF0P4HnAAXiBKw2Puc90Ggq4D5gENAM3Gh5z5dHOYbPZdGZmZnR+ACGESELNzc1aa50QFydRCyjgceAfwJOd1pUDCw2PWWE6jfLw8l3ARGBI+HU68ED4/YgyMzNpamqKcNlCCJG8lFItVtfQVVFLUcNjvgXUHLR6GvBE+PMTwPRO6580PKY2POZ/gQLTaRRFqzYhhBDxL9aXeX0Nj7k9/HkH0Df8uRio7rTdlvA6IYQQ3ZRl7ZCGx9TAMY+zpJSaoZRarpRa7vf7o1CZEEKIeBDrgNrZ0XQXft8VXr8VKO20XUl43RdorR/SWo/VWo9NSYnmLTQhhBBWinVAVQI3hD/fAMzttP5602ko02mcAdR1agoUQgjRDUWzm/lM4AKgl+k0tgC/BCqAF0yncTPwGXBlePP5hLqYbyDUzfymaNUlhBAiMahEnm4jOztbSzdzIYToOqVUs9Y62+o6uqJb3sR5+5PdLNlYQ6rdRmqKItVmI9WuSE2xhdbZVfjdRprdRsphltM69rfbQscIf06xKZRSVv+YQojjpLVGt7cTbGoi2NRMsLkJ3dxMsLmZQNPnn4PNzQSbmsFuw5aVjS07C1t2Nras8Ht2NvbsbFRW1v53+d3Qdd0yoJZtquH+NzcQjOLF4wEhl2LjrglOvjKmJHonFEKgtSawdy/tW7eGwuWgIPniusMvE41ewkphy8zcH14Hh9kBn4/wXWpJMba0tMjXF2e6dRNfIKhpDwTDL40/EMQX/tx5fXsgSLs/SHtQh97D2/k7becL79/xuWMff1DzurmT/MxU3LefG8GfXojuK+jz0b55M20bN+Lb5MW3cSNt3k34Nm4i2NBwxH1VVlbol33Hq+OX/5GWsw//ncrIgGDwwJDrCLpDfQ5fkQWbmkJXY03NBJo7b9eMbm4+4s/gmPUimaeeelz/30kTX4Kw2xR2m52MVHtUz1NckMn/zjf5bG8TA3smxL8XQlhOa02gpgbfpk1fCKL26i0QDO7fNqVPH9LKysib7CK9rIzUklLsuTkHNK3ZsrKxZWag7FH4791mw56Xhz0vLyKH08EgweaWwwRcE2mlpUc/SBLo1ldQsbJlXzPn/H4Rd01w8u0LTrK6HJGEtNbotrbQL7DGxv2/yAKNjaG/2DutCzY1EmhqAn8AW24u9txcbHnh99xc7Hl5n68Pv6soPnOofT58W7aEwmdT6CooFERegnV1+7dTaWmkORykDRpEWpmD9EGDSHOUkVbmwJ6TE7X6ko1cQYkDlBRmMaIkn5fXbpeAEoektcb36af499bsD5GOsAl0BEtj0xcDqOnzACIQ6NK5Ou5jYLeHjnWUJjEg1JyVm4s9Lxdbbh623BzsuXn7l+25OaH3vFxsOZ9vF3rPxZaejn/fPnybNh0YRJs24auuPqD2lN69Q1dDEyeQXlYWDqQyUouKonP1I+KWBFSMTBpexP+97KG6ppnSHllWlyPiRPvWrdRVVVE3txLfpk2H3kipUKjk5ITfQz3DUnr3wpad8/lN9JwcbNlZ2HMOXpf9+XZZmSjbgc/n60AgFHgNDQQaGgjU1+//HKxvINBQT7ChMfRe30CgsYHAnr34NnkJ1tcTaGw8eoeC1FRob//8R0pLI23gQNJPOYXcCZeGrobKykgrK5OrIbGfNPHFSHVNM+f+YRE/nejklvPlKqo7CzQ00LBgAXUvzaV5+XIAssaOJW/KFNIGDtwfQrbsbOw5OajMzLjumqy1Rre0hAKtoYFAfQPBhnoCDY2h9/oGgo2N2Hv22B9Eqf37y9WQRRKpiU8CKoam/P0dbDbF3NvOtroUEWO6vZ3Gd9+lbu5cGt9YhG5rI83hIH/aVPKmTCWtRAbvF7GRSAElTXwxNGl4Eb9/xcOWfc2UFEozX7LTWtO69iPqKiupd7sJ1NRgLyig4IoryJ8+jYzhw+P6ykgIq0lAxdCk4f34/SseXlm7g2+eO8jqckSUtG/bRl3VPOoqK/F9+ikqNZWcCy8kf/o0cs45B9UNHrAUIhIkoGJoYM9sTu2fh3vNdgmoJBNobKRhwavUVVbSvHQpaE3mmDH0+/WvyZtwKfb8fKtLFCLhSEDF2KThRdy7YD3balvoX5BpdTniBGi/n6b33qNubiUNCxeiW1tJHTiAXt+9jfypU7vNw5RCRIsEVIx1BNTLa3dw8zllVpcjjpHWmjbTpG5uJXVuN4E9e7Dn55N/2XTyp04lc+RIua8kRIRIQMVYWa9sjKI85q/ZLgGVQNp37qS+qoq6uXNp+2QDpKaSe8H55E+bRs5558l9JSGiQALKAq7h/fjjqx+zva6Fonxp5otXwaYm6l97jbq5c2n+75LQfaWRI+n3q1+SN2EC9oICq0sUIqlJQFlg4vAi/vjqx7yydgc3nS1XUfHGV13NvqefoXbWLIKNjaSWltLrO98hf2roQVohRGxIQFngpN45OPvlMn/NdgmoOKG1pnnJUmqeeorGN94Au528Sy6h8GvXkjl6tNxXEsICElAWmTisiL8u/Jid9a30zcuwupxuK9jaSv28edQ89TRt69djLyyk5y0zKLzmGlL79rW6PCG6NQkoi7hO68dfXg81891wlsPqcrqd9h072PfsTGpfeIFAbS3pp5xC0f/+ljyXC1uG/MEgRDyQgLLI4D65nNw3B/ea7RJQMaK1pmX1avY99RT1C14Frcn50oX0uO56ssaPk2Y8IeKMBJSFJg4r4m9vfMKuhlb65Mpf7dGifT7qFyyg5smnaF2zBltuLj2uv57Cr11LWkmJ1eUJIQ5DAspCrtOKuG/hJyxYu4PrznRYXU7S8e/Zw77nn2ffc88R2L2HtLIy+v7PLyiYNi00YZ8QIq5JQFno5L65DO6Tw/w1ElCR1LpuHTVPPkW9241ubyf7vHPp8bvryT77rC9M1ieEiF8SUBabNKwf/1i0gT2NbfTKSbe6nISl/X4aXl9IzVNP0bJiBSori4KvfpXCr3+d9EHSlV+IRCQBZbFJpxXxtzc28MraHXz9DHkI9FgFamupffFFap59Fv+27aSWlNCn/C4KLr8ce16e1eUJIU6ABJTFTumby6Be2by8drsE1DFo++QTap56mrrKSnRrK1mnn06/n/2MnAsukKnEhUgSElAWU0oxaXgR97+5gb2NbfSUZr7D0lrT9NZb1DzxBE3vvY9KTyd/6hQKv/51Mk45xeryhEh4jnJ3KfAk0BfQwEPeCtd9B22jgPuASUAzcKO3wrUyGvXIHeM4MGl4EUENCz7aaXUpcW3P/fdTfcuttH26kd533MHgNxdR9JvfSDgJETl+4IfeCtdQ4AzgNke5e+hB20wEhoRfM4AHolWMBFQcMIpycfTM4uW1260uJW7t+89/2PP3f5A/fTqDX3+NXrfMIKWw0OqyhEgq3grX9o6rIW+FqwEwgeKDNpsGPOmtcGlvheu/QIGj3F0UjXokoOJARzPfe5/upabJZ3U5cadh0SJ2/OrXZJ97LkW/uQeVmmp1SUIkshSl1PJOrxmH2shR7nYAo4AlB31VDFR3Wt7CF0MsIiSg4sSk4UUEgprX1u2wupS40rJ6NVvvuJMMw6Dkr3+RcBLixPm11mM7vR46eANHuTsHmAX8wFvhqo99iSESUHHi1P55DOiRhXuNBFSHto2bqL7126T06UPpvx6U0R+EiAFHuTuVUDg9461wzT7EJluB0k7LJeF1ESe9+OJERzPfw29vpLbZR0FW955CvH3XLqq/9S2w2Rjw8L9J6dnT6pKESHrhHnqPAKa3wvXnw2xWCXzXUe5+DjgdqPNWuKJyA10CKo5MGt6PBxd/yqvrdnLl2NKj75CkAo2NVN9yK/59+xj4xBOkDRhgdUlCdBdnA9cBaxzl7tXhdXcDAwC8Fa4HgfmEuphvINTN/KZoFaO01tE6dtRlZ2frpqYmq8uIGK015/5hEYP75PD4TeOtLscS2udj8y230LxsOaUP3E/OuedaXZIQSUUp1ay1Toj2crkHFUeUUriGF/Huhj3UNbdbXU7M6WCQbT/7Oc3v/5ei3/xGwkmIbk4CKs5MHF5Ee0Dzmtn9Htrd9ac/UV9VRe877qDgsulWlyOEsJgEVJwZUZJPcUEm89d0r4d2a558kppHHqXw2mvpOeNbVpcjhIgDElBxRinFxGH9ePuT3dS3do9mvvqXX2bn/1WQe8kl9P3Z3TL1uhACkICKS5NOCzXzvb4u+Zv5mv67hG0/uYvMMaPpf+8fZCRyIcR+lnQzN53GHcA3CY2Wu4ZQN8Ui4DmgJ7ACuM7wmN1y3J9RpQX0z89g/prtXD66xOpyoqZ1/Xq2fPe7pA4cQOk//4ktXUZyF0J8LuZXUKbTKAZuB8YaHnMYYAeuBn4P/MXwmIOBfcDNsa4tXiilmDCsiLc+3kNDkjbztW/bRvW3ZmDLzmbAv/+NPT/f6pKEEHHGqia+FCDTdBopQBawHfgS8GL4+yeA6daUFh9cp/XDFwiy0NxldSkRF6itZfO3ZhBsaaH03w+RWhSVgZCFEAku5gFleMytwB+BzYSCqY5Qk16t4TH94c0OOzquUmpGxyi8fr//UJskhVGlhfTLy0i63nzB1laqv/0d2jdvpuSf/yDj5JOtLkkIEaesaOIrJDSfSBnQH8gGJnR1f631Qx2j8KakJO9ITTabYsKwfrz58W4a25IjiHUgwNYf/YiW1avpf++9ZI/vnqNlCCG6xoomvi8DmwyPudvwmO3AbELjPxWEm/wgiqPjJhLXaUX4/EEWJsFDu1prdvzmNzS+vpC+d99N3oRLrS5JCBHnrAiozcAZptPIMp2GAi4C1gGLgK+Et7kBmGtBbXFlzIBC+uSm83ISTMGx98EHqX3ueXp+65v0uO7rVpcjhEgAVtyDWkKoM8RKQl3MbcBDwF3AnabT2ECoq/kjsa4t3thsoYd2F63fRVMCN/PVzprN7vv+Rv60qfS+806ryxFCJAgZzTzOLdm4l6se+i9/v2YUU0b0t7qcY9a4eDHV37mN7DPOoPTBB2RGXCEsJqOZi4gZ6+hBr5x0Xl6beL35Wj78kC0/uIMMp5Pi++6TcBJCHBMJqDhnDzfzveHZRbMvcZr5fF4v1bfcSkrPnpT+60HsOQnxB5sQIo5IQCWAicP70doe5M31u60upUv8e/aw+ZuhEckHPPxvUnr1srgiIUQikoBKAKeX9aRXThruBHhoN9DYRPWMW/Dv3Uvpvx4kzeGwuiQhRIKSgEoAdpvi0lP78Ya5ixZfwOpyDkv7fGz9/vdpXb+ekr/+hczTTrO6JCFEApOAShCThhfR0h5g8cfxOTaf1prtv/gFTe++S9E995Bz/vlWlySESHASUAni9LIe9MhOwx2nD+3u/vOfqZtbSe/v307BFZdbXY4QIglIQCWIFLst3My3k9b2+Grmq3nqafb++2EKrr6KnrfeanU5QogkIQGVQCYN70eTL8Dij+OnN1/9glfZ+bvfkfPli+j3i1/IdO1CiIiRgEogZwzqSWFWatxMwaHb29nx61+TMXw4xX/8o0zXLoSIKAmoBJJqt3HJ0H4sNHfFRTNf4+LFBGpq6PWdb2PLyLC6HCFEkpGASjCTTiuisc3P25/ssboUamfPIaV3b3LOOcfqUoQQSUgCKsGcdVJP8jOtb+bz79lD4+LF5E+bikriiSOFENaRgEowoWa+vry+bidtfuua+eoqqyAQIP9y6VIuhIgOCagENOm0Ihra/LxjUTOf1pq6ObPJHDGC9EGDLKlBCJH8JKAS0Nkn9SIvI4X5Fj2027p2LW2fbJCrJyFEVElAJaC0FBsXD+3Ha+t24PMHY37+2lmzUBkZ5E2aGPNzCyG6DwmoBOU6rR/1rX7e3RDbZr5gayv17vnkXnIx9tzcmJ5bCNG9SEAlqLMH9yI3PSXmvfkaXl9IsKGBAmneE0JEmQRUgkpPsXPx0L68um4n7YHYNfPVzZ5NanExWePHx+ycQojuSQIqgU0aXkRdS3vMmvnat22j6f33yZ8+HWWTf3WEENElv2US2DlDepGTnsLLMerNVzd3LmhN/mXTY3I+IUT3JgGVwDJS7XzZ6MOCdTui3syntaZ2zktknX46aSUlUT2XEEKABFTCmzi8iNrmdv67cW9Uz9OyfDntmzdTcPllUT2PEEJ0kEHUEtz5J/cmO83O/DXbOXdI76idp3b2HGzZ2eRecknUziGEsJ6j3P0oMBnY5a1wDTvE9xcAc4FN4VWzvRWue6JRiwRUgstItXOR0ZcFH+3kN9OCpNgjf1EcaGyifsEC8l2TsGVmRvz4Qoi48jjwD+DJI2zztrfCNTnahUgTXxKYNLwfNU0+lmyqicrxGxa8gm5ulqGNhOgGvBWut4Do/DI5RnIFlQQuOKUPWWl23Gu2c/bgXhE/fu3sOaSVlZE5cmTEjy2ESEhnOsrdHwDbgB95K1wfReMkcgWVBDJS7XzJ2YcFa3cQCOqIHtvn9dKyYgX5l1+GUiqixxZCWCJFKbW802vGMe6/EhjorXCNAP4OvBTxCsMkoJLEpOFF7G3ysWRTZHvz1c55CWw28qdOi+hxhRCW8Wutx3Z6PXQsO3srXPXeCldj+PN8INVR7o580w0SUEnjwlP6kJlqj+jYfDoQoO6ll8g+9xxS+/aJ2HGFEInLUe7u5yh3q/Dn8YRyJCrPucg9qCSRmWbnQmdvXlm7k19PHYbdduLNcU3vvY9/5076/vSnEahQCJEIHOXumcAFQC9HuXsL8EsgFcBb4XoQ+ArwbUe52w+0AFd7K1yRvbcQprSOynFjIjs7Wzc1NVldRtyY9+E2vvvsKp6bcQZnDOp5wsfbeuedNL37HoPffgtbWloEKhRCWE0p1ay1zra6jq6QJr4kcuEpfchItUWkmS9QV0fD6wvJmzJFwkkIYQkJqCSSnZ7CBSf34eUI9Oarc7vRPp8MbSSEsIwEVJKZPKKI3Q1tvPfpiU3BUTdrNulOJxlDh0aoMiGEODYSUEnm4qF9KcxKZebSzcd9jNb1H9P60Ucya64QwlISUEkmPcXOV8aU8OpHO9nd0HZcx6ibPRtSU8mbEvWhtoQQ4rCOGlCm05hiOg0JsgRy9fgB+IOaF1dsOeZ9dXs7dVVV5F54ISmFhVGoTgghuqYrwXMV8InpNP5gOg1ntAsSJ+6k3jmcXtaD55ZtJniMnSUaFy8mUFNDvnSOEEJY7KgBZXjMrwOjgE+Bx02n8b7pNGaYTiM36tWJ43bt6QP4bG8z7x/jRIa1s+eQ0rs3OeecE6XKhBCia7o0koThMetNp/EikAn8ALgM+LHpNP5meMy/H+tJTadRADwMDAM08A1gPfA84AC8wJWGx9x3rMcWIZee2o+CrFSeXbq5yyOc+/fsoXHxYnredCMqRQYZEUJYqyv3oKaaTmMO8Cah4S7GGx5zIjAC+OFxnvc+4BXDYzrDxzGBcmCh4TGHAAvDy+I4ZaTauWJ0Ca9+tIM9jV3rLFFXWQWBgMz7JISIC135M/kK4C+Gx3yr80rDYzabTuPmYz2h6TTygfOAG8PH8QE+02lMIzT+E8AThALxrmM9vvjcNeNLeeSdTcxasYVbzj/piNtqramdPYvMESNIHzQoRhUKIboLR7n7DOBXQAbwV2+F66Wj7dOVThK/ApZ2LJhOI9N0Gg4Aw2MuPI46y4DdwGOm01hlOo2HTaeRDfQ1PGbHGD07gL6H2lkpNaNjHhO/338cp+8+BvfJZbyjBzOXbuZoYy62rlmDb8OncvUkhIgIR7m730Gr7iR0e2gS8JuuHKMrAfUfINhpORBed7xSgNHAA4bHHAU0cVBznuExNaF7U1+gtX6oYx6TFLlPclRXjy/F24XOErWzZ6MyMsibNDFGlQkhktyDjnL3/zjK3Rnh5VpCI6FfBtR35QBdCaiUcDMcsL9J7kRGD90CbDE85pLw8ouEAmun6TSKAMLvu07gHCJs0vAi8jJSmLm0+rDbBFtbqXfPJ/eSi7HnSudMIcSJ81a4pgOrgHmOcvf1hDrYpQM9geldOUZXAmq36TSmdiyE7xUd90BvhsfcAVSbTuOU8KqLgHVAJXBDeN0NwNzjPYf4XEaqnctHl7Bg7Q72HqazRMPrCwk2NMjQRkKIiPJWuKqAS4F8YA7wsbfC9TdvhWt3V/Y/6nxQptM4CXgG6A8ooBq43vCYG463aNNpjCTUzTwN2AjcRCgsXwAGAJ8R6mZec6TjyHxQXbN+RwOX/vUtfjbJ4FvnfbEDxOZv3Izvs8846bVXUTYZNESIZBar+aAc5e6pwB2AH/gdoaupXwDFwM+8Fa5Pj3aMo97EMTzmp8AZptPICS83nkjR4WOsBsYe4quLTvTY4otO6ZfLmIGFzFy6mW+eW4ZSn8+2275tG03vv0+v73xHwkkIEUm/BcYTen52gbfCNR74oaPcPQT4X+Dqox2gS70MTKfhAk4FMkynAYDhMe85zqKFBa4ZP4Af/ecDlmyqOWC23bq5c0Fr8i+bbl1xQohkVAdcDmTRqU+Bt8L1CV0IJ+jag7oPEhqP73uEmvi+Cgw8jmKFhVzDi8jNSDlgGg6tNbVzXiLr9NNJKymxsDohRBK6jFCHiBTg2uM5QFeuoM4yPOZpptP40PCYvzadxp+Al4/nZMI6mWl2Lh9VzMxl1fyqyUdhdhoty5fTvnkzvW/7jtXlCSGSjLfCtQc45qHwOuvKTYfW8Huz6TT6A+1A0YmcVFjjmtMH4PMHmb1qKwC1s2Zjy84m95JLLK5MCCG+qCsBVRUe3PVeYCWhgVyfjWJNIkqc/fIYNaCAmUs3429opH7BAvImTcSWmWl1aUII8QVHbOILT1S40PCYtcAs02nMAzIMj1kXi+JE5F0zfgA/efFD3p71Kv1aWmRoIyFEVDnK3dlAi7fCFXSUu08GnMDL3gpX+9H2PeIVlOExg8A/Oy23STgltsmnFZGbnsLMFdtIKysjc+RIq0sSQiS3t4AMR7m7GHgVuA54vCs7dqWJb6HpNK4wnYY6+qYi3mWlpTBlcC6LMkpg2uUHPBMlhBBRoLwVrmZCXc7v91a4vkrosaWj6kpA3UJocNg202nUm06jwXQaXRroT8SnSdtX025PZZHjdKtLEUIkP+Uod58JfA1wh9fZu7JjV0aSkNFDk4gOBOjrfgFj3Dd5wczh5ku1XEUJIaLpB8BPgTneCtdHjnL3IGBRV3Y8akCZTuO8Q60/eAJDkRia3nsf/86dXHVqT361vpGVm/cxZmAPq8sSQiQpb4VrMbAYwFHutgF7vBWu27uyb1ce1P1xp88ZhMZWWgF86RjrFHGgbs5s7Pn5XPGV87n33rd4dkm1BJQQImoc5e5ngVsJzSW4DMhzlLvv81a47j3avke9B2V4zCmdXhcDw4B9J1q0iL1AbS0Nry8kb8oUcnOzmDaqmHkfbqOu+ai9PYUQ4ngN9Va46gnNAfUyoVnVr+vKjsczfPUWwDiO/YTF6txutM9HweWXAXDt+AG0+YO8tHqrxZUJIZJYqqPcnUoooCrDzz8deZ6nsK7cg/p7p4PZgJGERpQQCaZu9hzSnU4yhg4FYFhxPsOL85m5dDPXnzlQOksIIaLhX4RGIPoAeMtR7h5IF6d878o9qOWdPvuBmYbHfPdYKxTWal3/Ma0ffUTfu+8+YP014wdw95w1rKquZfSAQouqE0IkK2+F62/A3zqt+sxR7r6wK/t2JaBeBFoNjxkAMJ2G3XQaWYbHbD72UoVV6mbPhtRU8qZMPmD91JH9+a17HTOXbJaAEkJEnKPcnQ/8EujoEb4YuIfQfFFH1KWRJAjNiNghE3j9GGsUFtLt7dRVVZF74YWkFB4YQjnpKUwb2Z+qD7dR3yqdJYQQEfco0ABcGX7VA491ZceuXEFldJ7m3fCYjabTyDqeKoU1GhcvJlBTQ364c8TBrhk/gJlLq5m7aivXnemIbXFCiGR3krfCdUWn5V87yt2ru7JjV66gmkynMbpjwXQaY4CWY6tPWKl29hxSevcm55xzDvn98OJ8Tu2fxzNLNqN1lzrXCCFEV7U4yt37f/k4yt1n08UM6coV1A+A/5hOYxuhKd/7EZoCXiQA/549NC5eTM+bbkSlHPoft1KKa8YP4OcvreWDLXWMLC2IbZFCiGR2K/Bk+F4UhJ6jvaErO3blQd1lhObv+Hb4RIbhMVccZ6EixuoqqyAQOOq8T9NG9icz1c7MJZtjVJkQojvwVrg+8Fa4RgCnAad5K1yj6OJIRF15Duo24BnDY64NLxeaTuMaw2PefyJFi+jTWlM7exaZI0aQPmjQEbfNzUhl6ohQZ4mfTzbIzUiNUZVCiHjiKHc/CkwGdnkrXMMO8b0C7gMmAc3Ajd4K11GfjQ2PJtHhTuCvR9unK/egvhWeURcAw2PuA77Vhf2ExVrXrMG34dMuz5p7zekDaPYFqPxgW5QrE0LEsceBCUf4fiIwJPyaATxwHOfo0qgAXQkoe+fJCk2nYQfSjqMgEWO1s2ejMjLImzSxS9uPKMnHKMpj5lJp5hOiu/JWuN4Cao6wyTTgSW+FS3srXP8FChzl7qJjPE1khjoCXgGeN53Gv8LLtxAa8E/EsWBrK/Xu+eRecjH23K5N6aWU4trxpfxi7kes2VLH8JL8o+8khEg0KUqpziMEPaS1fugY9i8Gqjstbwmv2955I0e5u4FDB5HiwGdrD19oF7a5i9Bl3K3h5Q8J9eQTcazh9YUEGxoo6GLzXodpo4r53/kmzy7dzP+VDI9SdUIIC/m11mOjfRJvheuEJ7vtSi++ILCE0GB/4wn1vjBP9MQiuupmzya1uJis8eOPab+8jFQmn9afytVbaWzzR6k6IUQC2wqUdlouCa+LuMNeQZlO42TgmvBrD/A8gOExuzTIn7BO+7ZtNL3/Pr2+8x2U7dhnVLlm/ABeXLGFqg+2cc34AVGoUAiRwCqB7zrK3c8BpwN13grX9qPsc1yO1MTnAd4GJhsecwOA6TTuiEYRIrLq5s4Frcm/bPpx7T96QAGn9M1l5tLNElBCdDOOcvdM4AKgl6PcvYXQQK+pAN4K14PAfEJdzDcQ6mZ+U7RqUYcb2sZ0GtOBq4GzCXWUeA542PCYZdEq5lhlZ2frpqYmq8uIKzoY5NNLJ5Davz8Dn3j8uI/z+Lub+FXVOuZ97xyGFUtnCSGShVKqWWudbXUdXXHY9h/DY75keMyrCY0isYjQkEd9TKfxgOk0LolRfeIYtaxYQXt19f5Zc4/XZaNKSE+xSZdzIYRlutJJosnwmM8aHnMKoZthqwj17BNxqHbuXGxZWeRefPEJHSc/KxXXaUXMXb2NJuksIYSwwDHdQTc85j7DYz5keMyLolWQOH7BtjYaXllA7iWXYMs68RlRrh0/gMY2P/M+lJElhBCxd+xdvETcalz0JsHGRvKnTonI8cYMLGRInxyeXVp99I2FECLCJKCSSF1VFSm9e5N1+ukROV7HNBwfVNfy0bajzs4shBARJQGVJPz79tH41lvkTZ6MstsjdtzLRxeTlmLjObmKEkLEmARUkmhYsADa2yPWvNehICsN1/AiXlq1lWafdJYQQsSOBFSSqKusIn3IYNKdzogf+5rxA2ho8zPvw6g8LC6EEIckAZUEfNXVtKxcSd6UqSjVpWlWjsk4RyEn9c6WZ6KEEDElAZUE6ufNAyB/sisqx+/oLLFqcy3m9vqj7yCEEBFgWUCZTsNuOo1VptOYF14uM53GEtNpbDCdxvOm05BJEbtAa01dZRVZ48aR2r9/1M5zxegS0uw2npOrKCFEjFh5BfV9Dpy24/fAXwyPORjYB9xsSVUJpnXtR/g2bSIvwp0jDlaYncbE4f2Ys2orLb5AVM8lhBBgUUCZTqMEcAEPh5cVoXmmXgxv8gQw3YraEk1dVSUqNZW8Sy+N+rmuHjeA+lY/89dIZwkhRPRZdQX1V+AnQDC83BOoNTxmRz/mjimEv0ApNUMptVwptdzv797dnrXfT717PjkXXog9Ly/q5ztjUA/KeklnCSFEbMQ8oEynMRnYZXjMFcezv9b6Ia31WK312JSUrsxYn7ya3n+fwN69EX/26XBCnSVKWf7ZPj7e2RCTcwohui8rrqDOBqaaTsNLaI6pLwH3AQWm0+hInKhNIZxM6iqrsOfnk3PeeTE75xWjS0i1K7mKEkJEXcwDyvCYPzU8ZonhMR2EJkR8w/CYXyM059RXwpvdAMyNdW2JJNjURMPrr5M7cQIqLXYdHnvmpHPpqf2YvXIrre3SWUIIET3x9BzUXcCdptPYQOie1CMW1xPXGl5/Hd3SQv7UqTE/97XjB1DX0s7La6WzhBAieg475Xsi6M5Tvm+++Zv4vF5Oev21qIwecSTBoOZLf3qTPrkZvHDrmTE9txDixCTFlO8ifrXv2kXT+++TN3VKzMMJwGZTXD1+AEu9NWzYJZ0lhBDRIQGVgOrnz4dgkPwpsem9dyhfGdPRWUKm4RBCRIcEVAKqr6wiY9gw0gcNsqyGXjnpXDK0H7NWbpHOEkKIqJCASjBtGzbQum5dzJ59OpJrxg+gtrmdV9busLoUIUQSkoBKMHVV88BuJ2/SJKtL4ayTejKoVzZ3zfqQexd4aGzr3iN7CCEiSwIqgehgkPqqKrLPOouUXr2sLgebTfH0N09n4rB+/HPRp1xw7yKeXbIZfyB49J2FEOIoJKASSMvKlbRv2xYXzXsd+hdk8terR/HSbWdT1iubu+esYdLf3mbxx7utLk0IkeAkoBJIXWUVKiuL3IsusrqULxhZWsALt5zJA18bTZs/yA2PLuX6R5eyfod0QxdCHB95UDdBBH0+PjnnXHIuOJ/iP/zB6nKOqM0f4Kn3P+NvCz+hsc3PVeMGcOfFJ9M7N93q0oTo9uRBXRFxjYsXE6yvJ39K7Ic2OlbpKXa+ee4gFv/4Qm44y8F/lldzwb2L+OeiDdIlXQjRZXIFlSC2fO92mletYsibi1AJNs3Ixt2NVLzs4dV1O+mfn8GPJ5zCtBHF2GyxHwVDiO5OrqBERAXq6mh8803yXa6ECyeAQb1zeOj6sTw34wx65KRxx/MfMP3+d1m6qcbq0oQQcUwCKgHUv7IA3d5OXhz13jseZwzqSeVt5/DnK0ewq76NK//1Prc+tQLvnu5xFSyEODbSxJcAvF//OoF9tQyaV2XJ4LDR0OIL8PDbG3lg8ae0B4Jcd4aD2y8aTEFW7Oa2EqI7kiY+ETG+LVtpWb6C/CnWjFweLZlpdr530RDe/NEFXDG6hMff28T5977Jo+9swueXB32FEBJQca9+3jwA8iZPtriS6OiTl0HFFafhvv1cTivJ555567jkL4tZ8NEOEvnqXghx4qSJL45prdnomoy9RyGOp5+2upyo01rz5se7+Z3b5JNdjYwv68EvXEMZXpJvdWlCJA1p4hMR0bpuHb6NGxPi2adIUEpx4Sl9ePn75/K/lw3j012NTPnHO9z5/Gq21bZYXZ4QIsbkCiqO7fy/CvY9+yxD3nkbe373u4poaG3ngTc/5eF3NqGAGecN4pbzTyInPfG62gsRLxLpCkoCKk5pv59PLryQrJEjKfn7360ux1Jb9jVz74L1zF29jd656Tx24ziGFXe/wBYiEo4WUI5y9wTgPsAOPOytcFUc9P2NwL3A1vCqf3grXA9Ho1Zp4otTTf9dQmD3HvIsnNY9XpQUZnFfeMT0NLuNGx9bKs9OCREFjnK3HfgnMBEYClzjKHcPPcSmz3srXCPDr6iEE0hAxa36qkpseXnknH++1aXEjZGlBTx583iCGq57dAm76lutLkmIZDMe2OCtcG30Vrh8wHPANKuKkYCKQ8HmZupfe528Sy/Fli4jgHd2Uu8cHrtxHHsbfdzw2DLqWtqtLkmIRJOilFre6TWj03fFQHWn5S3hdQe7wlHu/tBR7n7RUe4ujVahElBxqGHhG+jm5riamDCejCgt4F/XjWHDrga+9eRyGSFdiGPj11qP7fR66Bj3rwIc3grXacBrwBORLzFEAioO1VVVktK/iMwxY6wuJW6dO6Q3f7pyJMu8Ndw+c5VMMy9EZGwFOl8RlfB5ZwgAvBWuvd4KV1t48WEgar+oJKDijH/PHprefY/8yVNQNvnHcyRTR/Tnl5OH8uq6nfz8pbUy8oQQJ24ZMMRR7i5zlLvTgKuBys4bOMrdRZ0WpwJmtIqRB0riTP38+RAISPNeF914dhl7m3z8/Y0N9MxJ48eXOq0uSYiE5a1w+R3l7u8CCwh1M3/UW+H6yFHuvgdY7q1wVQK3O8rdUwE/UAPcGK165DmoOLPpK1+FYJCy2bOsLiVhaK25e85aZi7dzC+nDOWms8usLkmIuJVID+pKG1Icadu4kda1axN+3qdYU0rx2+nDuPTUvvy6ah1zV289+k5CiLgnARVH6qqqwGYjb9Ikq0tJOHab4r6rR3F6WQ9++MIHLP54t9UlCSFOkARUnNBaU19ZRfaZZ5Lap4/V5SSkjFQ7/75hLEP65vLtp1ewurrW6pKEECdAAipOtKxaRfvWrdI54gTlZaTyxDfG0SsnnZseW8qGXY1WlySEOE4SUHGirrISlZlJ7pe/bHUpCa9PbgZP3Tweu01xw6NL2V4nU3UIkYgkoOKA9vmof/kVci+6CFt2QnSuiXsDe2bz+E3jqWtp5/pHllLb7LO6JCHEMZKAigONb79NsK5OmvcibFhxPg9dP4bP9jZz8xPLafHJkEhCJBIJqDhQV1mFvUcPss86y+pSks5ZJ/XivqtHsnLzPm57diXtMiSSEAlDAspigfp6GhctIs/lQqXIwB7RMHF4Eb+dPow3PLu4a9aHBIOJ+3C6EN2J/Ea0WMOrr6J9Pmnei7KvnT6QPQ0+/vL6x/TKSefuSYbVJQkhjkICymJ1lVWkORxkDBtmdSlJ7/aLBrO3qY2H3tpIr5w0Zpx3ktUlCSGOQALKQu3bttG8dCm9bv8eSimry0l6Sil+OeVU9jb5+N18Dz2y0/nKmBKryxJCHEbMA8p0GqXAk0BfQAMPGR7zPtNp9ACeBxyAF7jS8Jj7Yl1fLNW53QDkT5HmvVix2xR/vnIEtc0+7pr1IYVZqVxk9LW6LCHEIVjRScIP/NDwmEOBM4DbTKcxFCgHFhoecwiwMLyctEJDG1WSOXo0aaVRmzFZHEJ6ip1/XTeWoUV53PbsSlZ8VmN1SUKIQ4h5QBkec7vhMVeGPzcQmuyqGJjG51MHPwFMj3VtsdTm8dD2yQbpHGGRnPQUHrtpHEX5mXzj8eV8vLPB6pKEEAextJu56TQcwChgCdDX8Jjbw1/tINQEmLTqKqsgNZXcSy+1upRuq1dOOk9+YzzpKTauf2QpW/Y1W12SEKITywLKdBo5wCzgB4bHrO/8neExNaH7U1+glJqhlFqulFru9/tjUGnk6UCA+nnzyDnvPFIKC60up1sr7ZHFE98YT5PPz/WPLqWmSYZEEiJeWBJQptNIJRROzxgec3Z49U7TaRSFvy8Cdh1qX631Q1rrsVrrsSkJ+mBr85Il+Hfvls4RccIoyuORG8axdV8LNz22lKa2xPzDR4hkE/OAMp2GAh4BTMNj/rnTV5XADeHPNwBzY11brNRVVmHLySHnwgusLkWEjS/rwT+uHc3abfXc+vQKfH4ZEkkIqymtYzvsi+k0zgHeBtYAHb8F7iZ0H+oFYADwGaFu5kfsXpWdna2bmpqiWG3kBVta+OTsc8idNJH+v/2t1eWIg7ywrJqfzPqQqSP689erRmKzyfNpIrkopZq11gkxbULM28gMj/kOcLj/6i+KZS1WaHjjDYLNzeRPmWp1KeIQrhxXyp6mNv7wynp6ZKfxyylD5SFqISySmDdxElh9ZRUp/fqRNW6s1aWIw/j2+Sexp8HHo+9uYndjG4N6ZZOXkUpuRgp5mankZaSSl5kSfg+tT7XLuMtCRJoEVAz5a2pofOcden7jJpRNfqHFK6UUP3cZtAeCzF29lflrtnO0lvDMVPsBoZXXKcyOFGwd69JT7LH54YRIIDG/BxVJiXYPqubpZ9j5299SVjmXjJNPtroc0UXBoKbJ56e+1U99SzsN4ff61vbwe6f1rR3rQ587tvUfZYqP9BQbBVmpXHhKH64aV8rI0gJpWhRRkUj3oCSgYmjTVVeh23wMemmO1aWIGNJa09Ie6BRanwfY/oBrbWd7bSuvrdtJS3uAk/vmcNW4AVw2qpge2WlW/wgiiUhAxUgiBZTP6+XTCRPp8+Mf0/Pmb1hdjohTjW1+5n2wjeeWVbO6upY0u42LT+3LVWNLOWdwL+lVKE6YBFSMJFJA7f77P9hz//0MfnMRqX2TehQnESHrdzTw/LJqZq/aQm1zO8UFmVw5tpSvji2hf0Gm1eWJBCUBFSOJElBaaz69dAKpxf0Z+NhjVpcjEkybP8Br63by/LJq3v5kD0rBeUN6c9W4Ur5s9CUtRTrciK6TgIqRRAmo5lWr+Oyaayn63e8ouPwyq8sRCay6ppn/rNjCf5ZXs72ulZ7ZaVw+upirxpUyuE+u1eWJBCABFSOJElA77rmH2tlzGPLO29hzcqwuRySBQFDz1ie7eWFZNa+t24k/qBkzsJCrxpbiOq2I7HR5gkQcmgRUjMRrQLXv3EXLqpU0r1hJy4oVtHo85E2YQPGf/2R1aSIJ7WlsY/bKLTy/rJpPdzeRnWZn6sj+XDnWmu7qwaBmV0MbW/Y1s2VfC9U1zWyrayE7LYXiwkz6F2RSXBB6L8xKle70MSYBFSPxEFA6GMS3cWMojFauoHnFStq3bAFAZWaSOWIEWaNHUXjddTK1hogqrTUrPtvH88uqmffhdlraA5zSN5crx5VGtLt6MKjZ3fh5AHWEUOhzM9tqW/EFDhxst2d2Gk0+P63tB67PSLV9Hlj5odDqX5CxP8D65WeQkSoPMUeSBFSMWBFQQZ+P1rVraV6xgpYVK2letYpgXR0A9l69yBo9mszRo8gaM4YMpxOVmhrT+oQAaGhtp+qD7Ty/vJoPOnVXv3pcKWefdOTu6sGgZk9jG9XhwOkIoY7PW/e1fCGAeuWkUVyYRWlhJiWFWZQUZoZfWRQXZJKZZkdrzb7mdrbVtrC1toVt4dfW2ha21rayrbaF3Q1tX6inV046xQUZ4fDquALLoLggi/4FGfTITpOrsGMgARUjsQioQG0tzatW0bIy1GTXunYt2hea1C6trIzMMaPJGj2GrDGjSR0wQP5DEXHHs6Oe55dVM2fV1gO6q595Uk921LceGEI1zWypbfnCdCM9s9P2B05Jj89DqLQwk+KCLDLTInOV0+YPsKOuNRxgrQeEWMf7wVdh6Sm2/Vdc/cNB1js3HZtS2BQoFOH/oZRCATZbaH3Hf67716vQutC2QHgbW/h7pcKvTse0KUVORgq9c9LpnZse91d8ElAxEumA0lrTvnUrLStCTXUtq1bS9smG0JepqWQOHUrmmFAYZY4aRUqPHhE7txDR1toe4NV1O3lhWTXvbNhzwHc99gdQKHw6XwkVF2aSlRYfnS601tQ2t4evuj6/CttW27p/edchrsJiKS8jhT55GfsDq09u+D0vnd45GfvXFVh0/00CKkZONKC030/r+vXhprqVtKxYiX9XaCJfW24umaNG7r86yhg+HFtGRqRKF8JS1TXNbNjVSHFh6P5PMvX6a/MHqG1uJ6g1WrP/HUBr0OjweyjwQsMkdl7XaZvw/ofbt+O9vsXP7oY2djW0ht/b9r/vamj9wlUfQKpd7Q+x0CvjgEDr+NwrJ7JXZRJQMXK8AVX/yivUvvAfWlavJtjcDEBK/6L9YZQ5ejTpgwej7PF9qS6EiH9aa5p8AXbVfzG8OofansY29jb5Djlyfn5maii0ckJXYndefDIDex5fxiRSQCXPn03HwL9zJ/6aGvKnTydz9GiyRo8itX9/q8sSQiQhpRQ56Snk9M5hUO8jPwfZHghS0+RjV30buxtbQ+8NbexubAuva2Pl5n0cZXD8pNEtr6C01tKZQQjRLckVVJyTcBJCiENzlLsnAPcBduBhb4Wr4qDv04EngTHAXuAqb4XLG41aZJRJIYQQADjK3Xbgn8BEYChwjaPcPfSgzW4G9nkrXIOBvwC/j1Y9ElBCCCE6jAc2eCtcG70VLh/wHDDtoG2mAU+EP78IXOQod0elWUoCSgghupcUpdTyTq8Znb4rBqo7LW8Jr+NQ23grXH6gDugZlUKjcVAhhBBxy6+1Hmt1EV0hV1BCCCE6bAVKOy2XhNcdchtHuTsFyCfUWSLi5ApKCCFEh2XAEEe5u4xQEF0NXHvQNpXADcD7wFeAN7wVrqg8ryRXUEIIIYD995S+CywATOAFb4XrI0e5+x5HuXtqeLNHgJ6OcvcG4E6gPFr1dMsHdYUQortKpAd1EzqglFJBoOU4d08B/BEsJ57Iz5a4kvnnk58tPmRqrROi9SyhA+pEKKWWJ0pPlmMlP1viSuafT342cawSIkWFEEJ0PxJQQggh4lJ3DqiHrC4giuRnS1zJ/PPJzyaOSbe9ByWEECK+decrKCGEEHGsWwaUUmqCUmq9UmqDUipqD5nFmlKqVCm1SCm1Tin1kVLq+1bXFGlKKbtSapVSap7VtUSSUqpAKfWiUsqjlDKVUmdaXVOkKKXuCP/7uFYpNVMplWF1TSdCKfWoUmqXUmptp3U9lFKvKaU+Cb8XWlljsuh2AaWU+sJ8J0qpg+c7SVR+4Ida66HAGcBtSfSzdfg+oSfck819wCtaaycwgiT5GZVSxcDtwFit9TBCk+BdbW1VJ+xxYMJB68qBhVrrIcBCoji6QnfS7QKK8HwnWuuNWuvDzXeSkLTW27XWK8OfGwj9kjt4qPyEpZQqAVzAw1bXEklKqXzgPEJDyKC19mmtay0tKrJSgEylVAqQBWyzuJ4TorV+C6g5aHXnOZKeAKbHsqZk1R0DqivznSQ8pZQDGAUssbiUSPor8BMgaHEdkVYG7AYeCzdfPqyUSoihaI5Ga70V+COwGdgO1GmtX7W2qqjoq7XeHv68A+hrZTHJojsGVNJTSuUAs4AfaK3rra4nEpRSk4FdWusVVtcSBSnAaOABrfUooIkkaSIK34uZRiiE+wPZSqmvW1tVdOlQ12jpHh0B3TGgujLfScJSSqUSCqdntNazra4ngs4GpiqlvISaZb+klHra2pIiZguwRWvdcbX7IqHASgZfBjZprXdrrduB2cBZFtcUDTuVUkUA4fddFteTFLpjQC0DhiilypRSaYRu2FZaXFNEKKUUofsYptb6z1bXE0la659qrUu01g5C/8ze0FonxV/iWusdQLVS6pTwqouAdRaWFEmbgTOUUlnhfz8vIkk6gBykY44kwu9zLawlaXS7CQu11n6lVMd8J3bgUa31RxaXFSlnA9cBa5RSq8Pr7tZaz7euJNFF3wOeCf/RtBG4yeJ6IkJrvUQp9SKwklAv01Uk+KgLSqmZwAVAL6XUFuCXQAXwglLqZuAz4ErrKkweMpKEEEKIuNQdm/iEEEIkAAkoIYQQcUkCSgghRFySgBJCCBGXJKCEEELEJQkoIYQQcUkCSiQtpVRj+N2hlLo2wse++6Dl9yJ5/EOcb7pS6n+O8P1wpdTj0axBiFiTgBLdgQM4poAKj7x9JAcElNY62sP3/AS4/3Bfaq3XACVKqQFRrkOImJGAEt1BBXCuUmp1ePI8u1LqXqXUMqXUh0qpWwCUUhcopd5WSlUSHmpIKfWSUmpFeMK9GeF1FYSmj1itlHomvK7jak2Fj71WKbVGKXVVp2O/2WlSwmfCQ/+glKoITzL5oVLqjwcXr5Q6GWjTWu8JL381fPwPlFJvddq0isSfa0mI/brdUEeiWyoHfqS1ngwQDpo6rfU4pVQ68K5SqmMKiNHAMK31pvDyN7TWNUqpTGCZUmqW1rpcKfVdrfXIQ5zrcmAkoUkHe4X36QiRUcCphOZDehc4WyllApcBTq21VkoVHOKYZxMaKqjD/wCXaq23HrT98vDP+oeu/d8iRHyTKyjRHV0CXB8er3AJ0BMYEv5uaadwArhdKfUB8F9Co+AP4cjOAWZqrQNa653AYmBcp2Nv0VoHgdWEmh7rgFbgEaXU5UDzIY5ZRGi+qA7vAo8rpb5FaDzJDrsITWkhRFKQgBLdkQK+p7UeGX6VdZpEr2n/RkpdQGi6iDO11iMIDXSacQLnbev0OQCkaK39hGZ5fhGYDLxyiP1aOp9Xa30r8HNCgblCKdUz/FVGeFshkoIElOgOGoDcTssLgG+H585CKXXyYWawzQf2aa2blVJO4IxO37V37H+Qt4Grwve5ehOayn3p4QoLTy6ZHx5x/g5CTYMHM4HBnfY5SWu9RGv9P4SurDrmNzsZWHu4cwmRaOQelOgOPgQC4aa6x4H7CDWvrQx3VNgNTD/Efq8At4bvE60n1MzX4SHgQ6XUSq311zqtnwOcCXxAaFbVn2itd4QD7lBygblKqQxCV3Z3HmKbt4A/KaVUeLbWe5VSQ8LbLwyfC+BCwH3Y/xeESDAy3YYQCUApdR9QpbV+/TDfpxO633VOuNlQiIQnTXxCJIbfAVlH+H4AUC7hJJKJXEEJIYSIS3IFJYQQIi5JQAkhhIhLElBCCCHikgSUEEKIuCQBJYQQIi79PxlHxRZGi9bdAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Iterations (s)')\n",
    "ax1.set_ylabel('Accuracy', color=color)\n",
    "ax1.plot(accuracyArrNoGradAccum, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Loss %', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(train_lossesNoGradAccum, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 2.304459810256958. Accuracy: 11.350000381469727\n"
     ]
    }
   ],
   "source": [
    "lstmModel(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n"
     ]
    }
   ],
   "source": [
    "print(300 / (len(train_dataset) / 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n"
     ]
    }
   ],
   "source": [
    "print(n_iters / (len(train_dataset) / 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPU:0\\nprocess      28952 uses     2459.000 MB GPU memory'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.list_gpu_processes(device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 64           |        cudaMalloc retries: 66        |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |  117757 KB |    2163 MB |   11996 GB |   11996 GB |\\n|       from large pool |  113159 KB |    2160 MB |   11956 GB |   11956 GB |\\n|       from small pool |    4598 KB |       9 MB |      40 GB |      40 GB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |  117757 KB |    2163 MB |   11996 GB |   11996 GB |\\n|       from large pool |  113159 KB |    2160 MB |   11956 GB |   11956 GB |\\n|       from small pool |    4598 KB |       9 MB |      40 GB |      40 GB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |    1678 MB |    2196 MB |    8794 MB |    7116 MB |\\n|       from large pool |    1672 MB |    2190 MB |    8758 MB |    7086 MB |\\n|       from small pool |       6 MB |      10 MB |      36 MB |      30 MB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |  148483 KB |    1623 MB |   31448 GB |   31448 GB |\\n|       from large pool |  146937 KB |    1620 MB |   31403 GB |   31403 GB |\\n|       from small pool |    1546 KB |       6 MB |      44 GB |      44 GB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |      33    |      60    |     836 K  |     836 K  |\\n|       from large pool |      10    |      25    |     499 K  |     499 K  |\\n|       from small pool |      23    |      51    |     336 K  |     336 K  |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |      33    |      60    |     836 K  |     836 K  |\\n|       from large pool |      10    |      25    |     499 K  |     499 K  |\\n|       from small pool |      23    |      51    |     336 K  |     336 K  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       7    |      11    |      42    |      35    |\\n|       from large pool |       4    |       8    |      24    |      20    |\\n|       from small pool |       3    |       5    |      18    |      15    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      12    |      23    |  485170    |  485158    |\\n|       from large pool |       2    |      12    |  328935    |  328933    |\\n|       from small pool |      10    |      17    |  156235    |  156225    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary(device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22332/2736999428.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# Getting gradients w.r.t. parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccumulation_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dis-sys_lstm/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dis-sys_lstm/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "# def lstmModel(batch_size):\n",
    "    # torch.cuda.set_per_process_memory_fraction(0.5, 0)\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                        train=False, \n",
    "                        transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "accumulation_steps = 10\n",
    "batch_size = 10000 // accumulation_steps\n",
    "# n_iters = 1000\n",
    "n_iters = 3000\n",
    "## note the len of the train_dataset is 60000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                        batch_size=batch_size, \n",
    "                                        shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                        batch_size=batch_size, \n",
    "                                        shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # Building your LSTM\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        #######################\n",
    "        #  USE GPU FOR MODEL  #\n",
    "        #######################\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        \n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        \n",
    "        # One time step\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        \n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28\n",
    "hidden_dim = 100\n",
    "layer_dim = 3  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\n",
    "output_dim = 10\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "    \n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "\n",
    "# Number of steps to unroll\n",
    "seq_dim = 28  \n",
    "\n",
    "## accumulation_steps\n",
    "# accumulation_steps = 10\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        #######################\n",
    "        #  USE GPU FOR MODEL  #\n",
    "        #######################\n",
    "        images = images.view(-1, seq_dim, input_dim).requires_grad_().to(device)\n",
    "        labels = labels.to(device)\n",
    "            \n",
    "        # # Clear gradients w.r.t. parameters\n",
    "        # optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size() --> 100, 10\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels) / accumulation_steps\n",
    "        \n",
    "        ## this is potentially needed to normalize our loss if our loss is averaged\n",
    "        # loss = loss / accumulation_steps\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        if ((i + 1) % accumulation_steps == 0) or (i + 1 == len(train_dataset)):\n",
    "        # if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "            optimizer.step()                            # Now we can do an optimizer step\n",
    "            optimizer.zero_grad()                           # Reset gradients tensors\n",
    "\n",
    "        # Updating parameters\n",
    "        # optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "            \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                images = images.view(-1, seq_dim, input_dim).to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n",
    "            # model.zero_grad()                           # Reset gradients tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results without grad accum and\n",
    "\n",
    "Iteration: 500. Loss: 2.2964327335357666. Accuracy: 11.350000381469727\n",
    "\n",
    "Iteration: 1000. Loss: 2.2962512969970703. Accuracy: 11.359999656677246\n",
    "\n",
    "Iteration: 1500. Loss: 1.9343382120132446. Accuracy: 29.65999984741211\n",
    "\n",
    "Iteration: 2000. Loss: 0.7669856548309326. Accuracy: 72.06999969482422\n",
    "\n",
    "Iteration: 2500. Loss: 0.4916616678237915. Accuracy: 85.75\n",
    "\n",
    "Iteration: 3000. Loss: 0.1785622239112854. Accuracy: 93.44999694824219"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results with gradient accum without / by accum_step\n",
    "\n",
    "Iteration: 500. Loss: 2.301828145980835. Accuracy: 11.350000381469727\n",
    "\n",
    "Iteration: 1000. Loss: 2.2957465648651123. Accuracy: 11.350000381469727\n",
    "\n",
    "Iteration: 1500. Loss: 2.096222162246704. Accuracy: 17.270000457763672\n",
    "\n",
    "Iteration: 2000. Loss: 1.7537957429885864. Accuracy: 37.02000045776367\n",
    "\n",
    "Iteration: 2500. Loss: 1.263187050819397. Accuracy: 55.880001068115234\n",
    "\n",
    "Iteration: 3000. Loss: 0.9184436798095703. Accuracy: 69.97000122070312"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next one is the comparsion of the lstm without grad accum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 364.00 MiB (GPU 0; 3.95 GiB total capacity; 344.12 MiB already allocated; 80.75 MiB free; 480.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_99266/1898803548.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# Forward pass to get output/logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m# outputs.size() --> 100, 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;31m# Calculate Loss: softmax --> cross entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dis-sys_lstm/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_99266/1898803548.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# One time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Index hidden state of last time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dis-sys_lstm/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dis-sys_lstm/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    692\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 364.00 MiB (GPU 0; 3.95 GiB total capacity; 344.12 MiB already allocated; 80.75 MiB free; 480.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "# from pytorch_memlab import MemReporter\n",
    "# import cProfile\n",
    "# import re\n",
    "\n",
    "# cProfile.run('re.compile(\"foo|bar\")')\n",
    "\n",
    "# def lstmModel(batch_size):\n",
    "    # torch.cuda.set_per_process_memory_fraction(0.5, 0)\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                        train=False, \n",
    "                        transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "accumulation_steps = 10\n",
    "batch_size = 2000\n",
    "#  // accumulation_steps\n",
    "# batch_size = 4000\n",
    "# n_iters = 1000\n",
    "n_iters = 3000\n",
    "## note the len of the train_dataset is 60000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                        batch_size=batch_size, \n",
    "                                        shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                        batch_size=batch_size, \n",
    "                                        shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # Building your LSTM\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        #######################\n",
    "        #  USE GPU FOR MODEL  #\n",
    "        #######################\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        \n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        \n",
    "        # One time step\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        \n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28\n",
    "hidden_dim = 100\n",
    "layer_dim = 3  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\n",
    "output_dim = 10\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "# reporter = MemReporter(model)\n",
    "# reporter.report()\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "    \n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "\n",
    "# Number of steps to unroll\n",
    "seq_dim = 28  \n",
    "\n",
    "## accumulation_steps\n",
    "# accumulation_steps = 10\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        #######################\n",
    "        #  USE GPU FOR MODEL  #\n",
    "        #######################\n",
    "        images = images.view(-1, seq_dim, input_dim).requires_grad_().to(device)\n",
    "        labels = labels.to(device)\n",
    "            \n",
    "        # # Clear gradients w.r.t. parameters\n",
    "        # optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size() --> 100, 10\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels) \n",
    "        # / accumulation_steps\n",
    "        \n",
    "        ## this is potentially needed to normalize our loss if our loss is averaged\n",
    "        # loss = loss / accumulation_steps\n",
    "        \n",
    "        # reporting memory usage\n",
    "#         reporter.report()\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # reporting memory usage\n",
    "#         reporter.report()\n",
    "\n",
    "        # Gadient accumulation\n",
    "        if ((i + 1) % accumulation_steps == 0) or (i + 1 == len(train_dataset)):\n",
    "            optimizer.step()                            # Now we can do an optimizer step\n",
    "            optimizer.zero_grad()                           # Reset gradients tensors\n",
    "\n",
    "        # Updating parameters\n",
    "        # optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "            \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                images = images.view(-1, seq_dim, input_dim).to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "            # torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}. Epoch: {}'.format(iter, loss.item(), accuracy, epoch))\n",
    "            print(torch.cuda.memory_summary(abbreviated=False))\n",
    "            # model.zero_grad()                           # Reset gradients tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 2.290384292602539. Accuracy: 10.279999732971191\n"
     ]
    }
   ],
   "source": [
    "lstmModel(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN transition to LSTM\n",
    "- LSTM Models in PyTorch\n",
    "    - Model A: 1 Hidden Layer LSTM\n",
    "    - Model B: 2 Hidden Layer LSTM\n",
    "    - Model C: 3 Hidden Layer LSTM\n",
    "- Models Variation in **Code**\n",
    "    - Modifying only step 4\n",
    "- Ways to Expand Models **Capacity**\n",
    "    - More **hidden units**\n",
    "    - More **hidden layers**\n",
    "- **Cons** of Expanding Capacity\n",
    "    - Need more **data**\n",
    "    - Does not necessarily mean higher **accuracy**\n",
    "- **GPU** Code\n",
    "    - 2 things on GPU\n",
    "        - **model**\n",
    "        - **tensors**\n",
    "    - Modifying only **Step 3, 4 and 7**\n",
    "- **7 Step** Model Building Recap\n",
    "    - Step 1: Load Dataset\n",
    "    - Step 2: Make Dataset Iterable\n",
    "    - **Step 3: Create Model Class**\n",
    "    - **Step 4: Instantiate Model Class**\n",
    "    - Step 5: Instantiate Loss Class\n",
    "    - Step 6: Instantiate Optimizer Class\n",
    "    - **Step 7: Train Model**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "344ae177e219389e04275113089f4c9a52e04cbce716790cc4d0cbcaa3704a8f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('dis-sys_lstm': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
